{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec4a3089",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import h5py\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from main import EfficientNetB0\n",
    "import efficientnet.tfkeras as efn\n",
    "from keras import layers\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from keras.applications import imagenet_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7bdc2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.python.ops.numpy_ops.np_config as np_config\n",
    "np_config.enable_numpy_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "386ca38f",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "deep_prof_model = tf.keras.models.load_model('model', compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f238c206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 stem_conv (3, 3, 5, 32)\n",
      "1\n",
      "2 stem_bn (32,)\n",
      "4\n",
      "(32,)\n",
      "4 block1a_dwconv (3, 3, 32, 1)\n",
      "1\n",
      "5 block1a_bn (32,)\n",
      "4\n",
      "(32,)\n",
      "9 block1a_se_reduce (1, 1, 32, 8)\n",
      "2\n",
      "(8,)\n",
      "10 block1a_se_expand (1, 1, 8, 32)\n",
      "2\n",
      "(32,)\n",
      "12 block1a_project_conv (1, 1, 32, 16)\n",
      "1\n",
      "13 block1a_project_bn (16,)\n",
      "4\n",
      "(16,)\n",
      "14 block2a_expand_conv (1, 1, 16, 96)\n",
      "1\n",
      "15 block2a_expand_bn (96,)\n",
      "4\n",
      "(96,)\n",
      "17 block2a_dwconv (3, 3, 96, 1)\n",
      "1\n",
      "18 block2a_bn (96,)\n",
      "4\n",
      "(96,)\n",
      "22 block2a_se_reduce (1, 1, 96, 4)\n",
      "2\n",
      "(4,)\n",
      "23 block2a_se_expand (1, 1, 4, 96)\n",
      "2\n",
      "(96,)\n",
      "25 block2a_project_conv (1, 1, 96, 24)\n",
      "1\n",
      "26 block2a_project_bn (24,)\n",
      "4\n",
      "(24,)\n",
      "27 block2b_expand_conv (1, 1, 24, 144)\n",
      "1\n",
      "28 block2b_expand_bn (144,)\n",
      "4\n",
      "(144,)\n",
      "30 block2b_dwconv (3, 3, 144, 1)\n",
      "1\n",
      "31 block2b_bn (144,)\n",
      "4\n",
      "(144,)\n",
      "35 block2b_se_reduce (1, 1, 144, 6)\n",
      "2\n",
      "(6,)\n",
      "36 block2b_se_expand (1, 1, 6, 144)\n",
      "2\n",
      "(144,)\n",
      "38 block2b_project_conv (1, 1, 144, 24)\n",
      "1\n",
      "39 block2b_project_bn (24,)\n",
      "4\n",
      "(24,)\n",
      "42 block3a_expand_conv (1, 1, 24, 144)\n",
      "1\n",
      "43 block3a_expand_bn (144,)\n",
      "4\n",
      "(144,)\n",
      "45 block3a_dwconv (5, 5, 144, 1)\n",
      "1\n",
      "46 block3a_bn (144,)\n",
      "4\n",
      "(144,)\n",
      "50 block3a_se_reduce (1, 1, 144, 6)\n",
      "2\n",
      "(6,)\n",
      "51 block3a_se_expand (1, 1, 6, 144)\n",
      "2\n",
      "(144,)\n",
      "53 block3a_project_conv (1, 1, 144, 40)\n",
      "1\n",
      "54 block3a_project_bn (40,)\n",
      "4\n",
      "(40,)\n",
      "55 block3b_expand_conv (1, 1, 40, 240)\n",
      "1\n",
      "56 block3b_expand_bn (240,)\n",
      "4\n",
      "(240,)\n",
      "58 block3b_dwconv (5, 5, 240, 1)\n",
      "1\n",
      "59 block3b_bn (240,)\n",
      "4\n",
      "(240,)\n",
      "63 block3b_se_reduce (1, 1, 240, 10)\n",
      "2\n",
      "(10,)\n",
      "64 block3b_se_expand (1, 1, 10, 240)\n",
      "2\n",
      "(240,)\n",
      "66 block3b_project_conv (1, 1, 240, 40)\n",
      "1\n",
      "67 block3b_project_bn (40,)\n",
      "4\n",
      "(40,)\n",
      "70 block4a_expand_conv (1, 1, 40, 240)\n",
      "1\n",
      "71 block4a_expand_bn (240,)\n",
      "4\n",
      "(240,)\n",
      "73 block4a_dwconv (3, 3, 240, 1)\n",
      "1\n",
      "74 block4a_bn (240,)\n",
      "4\n",
      "(240,)\n",
      "78 block4a_se_reduce (1, 1, 240, 10)\n",
      "2\n",
      "(10,)\n",
      "79 block4a_se_expand (1, 1, 10, 240)\n",
      "2\n",
      "(240,)\n",
      "81 block4a_project_conv (1, 1, 240, 80)\n",
      "1\n",
      "82 block4a_project_bn (80,)\n",
      "4\n",
      "(80,)\n",
      "83 block4b_expand_conv (1, 1, 80, 480)\n",
      "1\n",
      "84 block4b_expand_bn (480,)\n",
      "4\n",
      "(480,)\n",
      "86 block4b_dwconv (3, 3, 480, 1)\n",
      "1\n",
      "87 block4b_bn (480,)\n",
      "4\n",
      "(480,)\n",
      "91 block4b_se_reduce (1, 1, 480, 20)\n",
      "2\n",
      "(20,)\n",
      "92 block4b_se_expand (1, 1, 20, 480)\n",
      "2\n",
      "(480,)\n",
      "94 block4b_project_conv (1, 1, 480, 80)\n",
      "1\n",
      "95 block4b_project_bn (80,)\n",
      "4\n",
      "(80,)\n",
      "98 block4c_expand_conv (1, 1, 80, 480)\n",
      "1\n",
      "99 block4c_expand_bn (480,)\n",
      "4\n",
      "(480,)\n",
      "101 block4c_dwconv (3, 3, 480, 1)\n",
      "1\n",
      "102 block4c_bn (480,)\n",
      "4\n",
      "(480,)\n",
      "106 block4c_se_reduce (1, 1, 480, 20)\n",
      "2\n",
      "(20,)\n",
      "107 block4c_se_expand (1, 1, 20, 480)\n",
      "2\n",
      "(480,)\n",
      "109 block4c_project_conv (1, 1, 480, 80)\n",
      "1\n",
      "110 block4c_project_bn (80,)\n",
      "4\n",
      "(80,)\n",
      "113 block5a_expand_conv (1, 1, 80, 480)\n",
      "1\n",
      "114 block5a_expand_bn (480,)\n",
      "4\n",
      "(480,)\n",
      "116 block5a_dwconv (5, 5, 480, 1)\n",
      "1\n",
      "117 block5a_bn (480,)\n",
      "4\n",
      "(480,)\n",
      "121 block5a_se_reduce (1, 1, 480, 20)\n",
      "2\n",
      "(20,)\n",
      "122 block5a_se_expand (1, 1, 20, 480)\n",
      "2\n",
      "(480,)\n",
      "124 block5a_project_conv (1, 1, 480, 112)\n",
      "1\n",
      "125 block5a_project_bn (112,)\n",
      "4\n",
      "(112,)\n",
      "126 block5b_expand_conv (1, 1, 112, 672)\n",
      "1\n",
      "127 block5b_expand_bn (672,)\n",
      "4\n",
      "(672,)\n",
      "129 block5b_dwconv (5, 5, 672, 1)\n",
      "1\n",
      "130 block5b_bn (672,)\n",
      "4\n",
      "(672,)\n",
      "134 block5b_se_reduce (1, 1, 672, 28)\n",
      "2\n",
      "(28,)\n",
      "135 block5b_se_expand (1, 1, 28, 672)\n",
      "2\n",
      "(672,)\n",
      "137 block5b_project_conv (1, 1, 672, 112)\n",
      "1\n",
      "138 block5b_project_bn (112,)\n",
      "4\n",
      "(112,)\n",
      "141 block5c_expand_conv (1, 1, 112, 672)\n",
      "1\n",
      "142 block5c_expand_bn (672,)\n",
      "4\n",
      "(672,)\n",
      "144 block5c_dwconv (5, 5, 672, 1)\n",
      "1\n",
      "145 block5c_bn (672,)\n",
      "4\n",
      "(672,)\n",
      "149 block5c_se_reduce (1, 1, 672, 28)\n",
      "2\n",
      "(28,)\n",
      "150 block5c_se_expand (1, 1, 28, 672)\n",
      "2\n",
      "(672,)\n",
      "152 block5c_project_conv (1, 1, 672, 112)\n",
      "1\n",
      "153 block5c_project_bn (112,)\n",
      "4\n",
      "(112,)\n",
      "156 block6a_expand_conv (1, 1, 112, 672)\n",
      "1\n",
      "157 block6a_expand_bn (672,)\n",
      "4\n",
      "(672,)\n",
      "159 block6a_dwconv (5, 5, 672, 1)\n",
      "1\n",
      "160 block6a_bn (672,)\n",
      "4\n",
      "(672,)\n"
     ]
    }
   ],
   "source": [
    "for i,l in enumerate(deep_prof_model.layers):\n",
    "    if l.get_weights():\n",
    "        print(i, l.get_config()['name'], l.get_weights()[0].shape)\n",
    "        print(len(l.get_weights()))\n",
    "        try:\n",
    "            print(l.get_weights()[1].shape)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55346d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = efn.EfficientNetB0(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3323609",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 stem_conv (3, 3, 3, 32)\n",
      "2 stem_bn (32,)\n",
      "(32,)\n",
      "4 block1a_dwconv (3, 3, 32, 1)\n",
      "5 block1a_bn (32,)\n",
      "(32,)\n",
      "9 block1a_se_reduce (1, 1, 32, 8)\n",
      "(8,)\n",
      "10 block1a_se_expand (1, 1, 8, 32)\n",
      "(32,)\n",
      "12 block1a_project_conv (1, 1, 32, 16)\n",
      "13 block1a_project_bn (16,)\n",
      "(16,)\n",
      "14 block2a_expand_conv (1, 1, 16, 96)\n",
      "15 block2a_expand_bn (96,)\n",
      "(96,)\n",
      "17 block2a_dwconv (3, 3, 96, 1)\n",
      "18 block2a_bn (96,)\n",
      "(96,)\n",
      "22 block2a_se_reduce (1, 1, 96, 4)\n",
      "(4,)\n",
      "23 block2a_se_expand (1, 1, 4, 96)\n",
      "(96,)\n",
      "25 block2a_project_conv (1, 1, 96, 24)\n",
      "26 block2a_project_bn (24,)\n",
      "(24,)\n",
      "27 block2b_expand_conv (1, 1, 24, 144)\n",
      "28 block2b_expand_bn (144,)\n",
      "(144,)\n",
      "30 block2b_dwconv (3, 3, 144, 1)\n",
      "31 block2b_bn (144,)\n",
      "(144,)\n",
      "35 block2b_se_reduce (1, 1, 144, 6)\n",
      "(6,)\n",
      "36 block2b_se_expand (1, 1, 6, 144)\n",
      "(144,)\n",
      "38 block2b_project_conv (1, 1, 144, 24)\n",
      "39 block2b_project_bn (24,)\n",
      "(24,)\n",
      "42 block3a_expand_conv (1, 1, 24, 144)\n",
      "43 block3a_expand_bn (144,)\n",
      "(144,)\n",
      "45 block3a_dwconv (5, 5, 144, 1)\n",
      "46 block3a_bn (144,)\n",
      "(144,)\n",
      "50 block3a_se_reduce (1, 1, 144, 6)\n",
      "(6,)\n",
      "51 block3a_se_expand (1, 1, 6, 144)\n",
      "(144,)\n",
      "53 block3a_project_conv (1, 1, 144, 40)\n",
      "54 block3a_project_bn (40,)\n",
      "(40,)\n",
      "55 block3b_expand_conv (1, 1, 40, 240)\n",
      "56 block3b_expand_bn (240,)\n",
      "(240,)\n",
      "58 block3b_dwconv (5, 5, 240, 1)\n",
      "59 block3b_bn (240,)\n",
      "(240,)\n",
      "63 block3b_se_reduce (1, 1, 240, 10)\n",
      "(10,)\n",
      "64 block3b_se_expand (1, 1, 10, 240)\n",
      "(240,)\n",
      "66 block3b_project_conv (1, 1, 240, 40)\n",
      "67 block3b_project_bn (40,)\n",
      "(40,)\n",
      "70 block4a_expand_conv (1, 1, 40, 240)\n",
      "71 block4a_expand_bn (240,)\n",
      "(240,)\n",
      "73 block4a_dwconv (3, 3, 240, 1)\n",
      "74 block4a_bn (240,)\n",
      "(240,)\n",
      "78 block4a_se_reduce (1, 1, 240, 10)\n",
      "(10,)\n",
      "79 block4a_se_expand (1, 1, 10, 240)\n",
      "(240,)\n",
      "81 block4a_project_conv (1, 1, 240, 80)\n",
      "82 block4a_project_bn (80,)\n",
      "(80,)\n",
      "83 block4b_expand_conv (1, 1, 80, 480)\n",
      "84 block4b_expand_bn (480,)\n",
      "(480,)\n",
      "86 block4b_dwconv (3, 3, 480, 1)\n",
      "87 block4b_bn (480,)\n",
      "(480,)\n",
      "91 block4b_se_reduce (1, 1, 480, 20)\n",
      "(20,)\n",
      "92 block4b_se_expand (1, 1, 20, 480)\n",
      "(480,)\n",
      "94 block4b_project_conv (1, 1, 480, 80)\n",
      "95 block4b_project_bn (80,)\n",
      "(80,)\n",
      "98 block4c_expand_conv (1, 1, 80, 480)\n",
      "99 block4c_expand_bn (480,)\n",
      "(480,)\n",
      "101 block4c_dwconv (3, 3, 480, 1)\n",
      "102 block4c_bn (480,)\n",
      "(480,)\n",
      "106 block4c_se_reduce (1, 1, 480, 20)\n",
      "(20,)\n",
      "107 block4c_se_expand (1, 1, 20, 480)\n",
      "(480,)\n",
      "109 block4c_project_conv (1, 1, 480, 80)\n",
      "110 block4c_project_bn (80,)\n",
      "(80,)\n",
      "113 block5a_expand_conv (1, 1, 80, 480)\n",
      "114 block5a_expand_bn (480,)\n",
      "(480,)\n",
      "116 block5a_dwconv (5, 5, 480, 1)\n",
      "117 block5a_bn (480,)\n",
      "(480,)\n",
      "121 block5a_se_reduce (1, 1, 480, 20)\n",
      "(20,)\n",
      "122 block5a_se_expand (1, 1, 20, 480)\n",
      "(480,)\n",
      "124 block5a_project_conv (1, 1, 480, 112)\n",
      "125 block5a_project_bn (112,)\n",
      "(112,)\n",
      "126 block5b_expand_conv (1, 1, 112, 672)\n",
      "127 block5b_expand_bn (672,)\n",
      "(672,)\n",
      "129 block5b_dwconv (5, 5, 672, 1)\n",
      "130 block5b_bn (672,)\n",
      "(672,)\n",
      "134 block5b_se_reduce (1, 1, 672, 28)\n",
      "(28,)\n",
      "135 block5b_se_expand (1, 1, 28, 672)\n",
      "(672,)\n",
      "137 block5b_project_conv (1, 1, 672, 112)\n",
      "138 block5b_project_bn (112,)\n",
      "(112,)\n",
      "141 block5c_expand_conv (1, 1, 112, 672)\n",
      "142 block5c_expand_bn (672,)\n",
      "(672,)\n",
      "144 block5c_dwconv (5, 5, 672, 1)\n",
      "145 block5c_bn (672,)\n",
      "(672,)\n",
      "149 block5c_se_reduce (1, 1, 672, 28)\n",
      "(28,)\n",
      "150 block5c_se_expand (1, 1, 28, 672)\n",
      "(672,)\n",
      "152 block5c_project_conv (1, 1, 672, 112)\n",
      "153 block5c_project_bn (112,)\n",
      "(112,)\n",
      "156 block6a_expand_conv (1, 1, 112, 672)\n",
      "157 block6a_expand_bn (672,)\n",
      "(672,)\n",
      "159 block6a_dwconv (5, 5, 672, 1)\n",
      "160 block6a_bn (672,)\n",
      "(672,)\n",
      "164 block6a_se_reduce (1, 1, 672, 28)\n",
      "(28,)\n",
      "165 block6a_se_expand (1, 1, 28, 672)\n",
      "(672,)\n",
      "167 block6a_project_conv (1, 1, 672, 192)\n",
      "168 block6a_project_bn (192,)\n",
      "(192,)\n",
      "169 block6b_expand_conv (1, 1, 192, 1152)\n",
      "170 block6b_expand_bn (1152,)\n",
      "(1152,)\n",
      "172 block6b_dwconv (5, 5, 1152, 1)\n",
      "173 block6b_bn (1152,)\n",
      "(1152,)\n",
      "177 block6b_se_reduce (1, 1, 1152, 48)\n",
      "(48,)\n",
      "178 block6b_se_expand (1, 1, 48, 1152)\n",
      "(1152,)\n",
      "180 block6b_project_conv (1, 1, 1152, 192)\n",
      "181 block6b_project_bn (192,)\n",
      "(192,)\n",
      "184 block6c_expand_conv (1, 1, 192, 1152)\n",
      "185 block6c_expand_bn (1152,)\n",
      "(1152,)\n",
      "187 block6c_dwconv (5, 5, 1152, 1)\n",
      "188 block6c_bn (1152,)\n",
      "(1152,)\n",
      "192 block6c_se_reduce (1, 1, 1152, 48)\n",
      "(48,)\n",
      "193 block6c_se_expand (1, 1, 48, 1152)\n",
      "(1152,)\n",
      "195 block6c_project_conv (1, 1, 1152, 192)\n",
      "196 block6c_project_bn (192,)\n",
      "(192,)\n",
      "199 block6d_expand_conv (1, 1, 192, 1152)\n",
      "200 block6d_expand_bn (1152,)\n",
      "(1152,)\n",
      "202 block6d_dwconv (5, 5, 1152, 1)\n",
      "203 block6d_bn (1152,)\n",
      "(1152,)\n",
      "207 block6d_se_reduce (1, 1, 1152, 48)\n",
      "(48,)\n",
      "208 block6d_se_expand (1, 1, 48, 1152)\n",
      "(1152,)\n",
      "210 block6d_project_conv (1, 1, 1152, 192)\n",
      "211 block6d_project_bn (192,)\n",
      "(192,)\n",
      "214 block7a_expand_conv (1, 1, 192, 1152)\n",
      "215 block7a_expand_bn (1152,)\n",
      "(1152,)\n",
      "217 block7a_dwconv (3, 3, 1152, 1)\n",
      "218 block7a_bn (1152,)\n",
      "(1152,)\n",
      "222 block7a_se_reduce (1, 1, 1152, 48)\n",
      "(48,)\n",
      "223 block7a_se_expand (1, 1, 48, 1152)\n",
      "(1152,)\n",
      "225 block7a_project_conv (1, 1, 1152, 320)\n",
      "226 block7a_project_bn (320,)\n",
      "(320,)\n",
      "227 top_conv (1, 1, 320, 1280)\n",
      "228 top_bn (1280,)\n",
      "(1280,)\n",
      "232 probs (1280, 1000)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "for i,l in enumerate(model.layers):\n",
    "    if l.get_weights():\n",
    "        print(i, l.get_config()['name'], l.get_weights()[0].shape)\n",
    "        try:\n",
    "            print(l.get_weights()[1].shape)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b87f2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnet = EfficientNetB0(input_shape=[1, 3, 224, 224], num_channels=3)\n",
    "effnet1 = EfficientNetB0(input_shape=[1, 3, 224, 224],num_channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "945fd784",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([32, 3, 3, 3])\n",
      "batch_norm1.weight torch.Size([32])\n",
      "batch_norm1.bias torch.Size([32])\n",
      "batch_norm1.running_mean torch.Size([32])\n",
      "batch_norm1.running_var torch.Size([32])\n",
      "batch_norm1.num_batches_tracked torch.Size([])\n",
      "block_list.0.depth_conv.weight torch.Size([32, 1, 3, 3])\n",
      "block_list.0.batch_norm_depth.weight torch.Size([32])\n",
      "block_list.0.batch_norm_depth.bias torch.Size([32])\n",
      "block_list.0.batch_norm_depth.running_mean torch.Size([32])\n",
      "block_list.0.batch_norm_depth.running_var torch.Size([32])\n",
      "block_list.0.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.0.conv_se_reduce.weight torch.Size([8, 32, 1, 1])\n",
      "block_list.0.conv_se_reduce.bias torch.Size([8])\n",
      "block_list.0.conv_se_expand.weight torch.Size([32, 8, 1, 1])\n",
      "block_list.0.conv_se_expand.bias torch.Size([32])\n",
      "block_list.0.conv_output.weight torch.Size([16, 32, 1, 1])\n",
      "block_list.0.batch_norm_last.weight torch.Size([16])\n",
      "block_list.0.batch_norm_last.bias torch.Size([16])\n",
      "block_list.0.batch_norm_last.running_mean torch.Size([16])\n",
      "block_list.0.batch_norm_last.running_var torch.Size([16])\n",
      "block_list.0.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.1.conv_expand.weight torch.Size([96, 16, 1, 1])\n",
      "block_list.1.batch_norm_exp.weight torch.Size([96])\n",
      "block_list.1.batch_norm_exp.bias torch.Size([96])\n",
      "block_list.1.batch_norm_exp.running_mean torch.Size([96])\n",
      "block_list.1.batch_norm_exp.running_var torch.Size([96])\n",
      "block_list.1.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.1.depth_conv.weight torch.Size([96, 1, 3, 3])\n",
      "block_list.1.batch_norm_depth.weight torch.Size([96])\n",
      "block_list.1.batch_norm_depth.bias torch.Size([96])\n",
      "block_list.1.batch_norm_depth.running_mean torch.Size([96])\n",
      "block_list.1.batch_norm_depth.running_var torch.Size([96])\n",
      "block_list.1.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.1.conv_se_reduce.weight torch.Size([4, 96, 1, 1])\n",
      "block_list.1.conv_se_reduce.bias torch.Size([4])\n",
      "block_list.1.conv_se_expand.weight torch.Size([96, 4, 1, 1])\n",
      "block_list.1.conv_se_expand.bias torch.Size([96])\n",
      "block_list.1.conv_output.weight torch.Size([24, 96, 1, 1])\n",
      "block_list.1.batch_norm_last.weight torch.Size([24])\n",
      "block_list.1.batch_norm_last.bias torch.Size([24])\n",
      "block_list.1.batch_norm_last.running_mean torch.Size([24])\n",
      "block_list.1.batch_norm_last.running_var torch.Size([24])\n",
      "block_list.1.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.2.conv_expand.weight torch.Size([144, 24, 1, 1])\n",
      "block_list.2.batch_norm_exp.weight torch.Size([144])\n",
      "block_list.2.batch_norm_exp.bias torch.Size([144])\n",
      "block_list.2.batch_norm_exp.running_mean torch.Size([144])\n",
      "block_list.2.batch_norm_exp.running_var torch.Size([144])\n",
      "block_list.2.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.2.depth_conv.weight torch.Size([144, 1, 3, 3])\n",
      "block_list.2.batch_norm_depth.weight torch.Size([144])\n",
      "block_list.2.batch_norm_depth.bias torch.Size([144])\n",
      "block_list.2.batch_norm_depth.running_mean torch.Size([144])\n",
      "block_list.2.batch_norm_depth.running_var torch.Size([144])\n",
      "block_list.2.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.2.conv_se_reduce.weight torch.Size([6, 144, 1, 1])\n",
      "block_list.2.conv_se_reduce.bias torch.Size([6])\n",
      "block_list.2.conv_se_expand.weight torch.Size([144, 6, 1, 1])\n",
      "block_list.2.conv_se_expand.bias torch.Size([144])\n",
      "block_list.2.conv_output.weight torch.Size([24, 144, 1, 1])\n",
      "block_list.2.batch_norm_last.weight torch.Size([24])\n",
      "block_list.2.batch_norm_last.bias torch.Size([24])\n",
      "block_list.2.batch_norm_last.running_mean torch.Size([24])\n",
      "block_list.2.batch_norm_last.running_var torch.Size([24])\n",
      "block_list.2.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.3.conv_expand.weight torch.Size([144, 24, 1, 1])\n",
      "block_list.3.batch_norm_exp.weight torch.Size([144])\n",
      "block_list.3.batch_norm_exp.bias torch.Size([144])\n",
      "block_list.3.batch_norm_exp.running_mean torch.Size([144])\n",
      "block_list.3.batch_norm_exp.running_var torch.Size([144])\n",
      "block_list.3.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.3.depth_conv.weight torch.Size([144, 1, 5, 5])\n",
      "block_list.3.batch_norm_depth.weight torch.Size([144])\n",
      "block_list.3.batch_norm_depth.bias torch.Size([144])\n",
      "block_list.3.batch_norm_depth.running_mean torch.Size([144])\n",
      "block_list.3.batch_norm_depth.running_var torch.Size([144])\n",
      "block_list.3.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.3.conv_se_reduce.weight torch.Size([6, 144, 1, 1])\n",
      "block_list.3.conv_se_reduce.bias torch.Size([6])\n",
      "block_list.3.conv_se_expand.weight torch.Size([144, 6, 1, 1])\n",
      "block_list.3.conv_se_expand.bias torch.Size([144])\n",
      "block_list.3.conv_output.weight torch.Size([40, 144, 1, 1])\n",
      "block_list.3.batch_norm_last.weight torch.Size([40])\n",
      "block_list.3.batch_norm_last.bias torch.Size([40])\n",
      "block_list.3.batch_norm_last.running_mean torch.Size([40])\n",
      "block_list.3.batch_norm_last.running_var torch.Size([40])\n",
      "block_list.3.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.4.conv_expand.weight torch.Size([240, 40, 1, 1])\n",
      "block_list.4.batch_norm_exp.weight torch.Size([240])\n",
      "block_list.4.batch_norm_exp.bias torch.Size([240])\n",
      "block_list.4.batch_norm_exp.running_mean torch.Size([240])\n",
      "block_list.4.batch_norm_exp.running_var torch.Size([240])\n",
      "block_list.4.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.4.depth_conv.weight torch.Size([240, 1, 5, 5])\n",
      "block_list.4.batch_norm_depth.weight torch.Size([240])\n",
      "block_list.4.batch_norm_depth.bias torch.Size([240])\n",
      "block_list.4.batch_norm_depth.running_mean torch.Size([240])\n",
      "block_list.4.batch_norm_depth.running_var torch.Size([240])\n",
      "block_list.4.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.4.conv_se_reduce.weight torch.Size([10, 240, 1, 1])\n",
      "block_list.4.conv_se_reduce.bias torch.Size([10])\n",
      "block_list.4.conv_se_expand.weight torch.Size([240, 10, 1, 1])\n",
      "block_list.4.conv_se_expand.bias torch.Size([240])\n",
      "block_list.4.conv_output.weight torch.Size([40, 240, 1, 1])\n",
      "block_list.4.batch_norm_last.weight torch.Size([40])\n",
      "block_list.4.batch_norm_last.bias torch.Size([40])\n",
      "block_list.4.batch_norm_last.running_mean torch.Size([40])\n",
      "block_list.4.batch_norm_last.running_var torch.Size([40])\n",
      "block_list.4.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.5.conv_expand.weight torch.Size([240, 40, 1, 1])\n",
      "block_list.5.batch_norm_exp.weight torch.Size([240])\n",
      "block_list.5.batch_norm_exp.bias torch.Size([240])\n",
      "block_list.5.batch_norm_exp.running_mean torch.Size([240])\n",
      "block_list.5.batch_norm_exp.running_var torch.Size([240])\n",
      "block_list.5.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.5.depth_conv.weight torch.Size([240, 1, 3, 3])\n",
      "block_list.5.batch_norm_depth.weight torch.Size([240])\n",
      "block_list.5.batch_norm_depth.bias torch.Size([240])\n",
      "block_list.5.batch_norm_depth.running_mean torch.Size([240])\n",
      "block_list.5.batch_norm_depth.running_var torch.Size([240])\n",
      "block_list.5.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.5.conv_se_reduce.weight torch.Size([10, 240, 1, 1])\n",
      "block_list.5.conv_se_reduce.bias torch.Size([10])\n",
      "block_list.5.conv_se_expand.weight torch.Size([240, 10, 1, 1])\n",
      "block_list.5.conv_se_expand.bias torch.Size([240])\n",
      "block_list.5.conv_output.weight torch.Size([80, 240, 1, 1])\n",
      "block_list.5.batch_norm_last.weight torch.Size([80])\n",
      "block_list.5.batch_norm_last.bias torch.Size([80])\n",
      "block_list.5.batch_norm_last.running_mean torch.Size([80])\n",
      "block_list.5.batch_norm_last.running_var torch.Size([80])\n",
      "block_list.5.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.6.conv_expand.weight torch.Size([480, 80, 1, 1])\n",
      "block_list.6.batch_norm_exp.weight torch.Size([480])\n",
      "block_list.6.batch_norm_exp.bias torch.Size([480])\n",
      "block_list.6.batch_norm_exp.running_mean torch.Size([480])\n",
      "block_list.6.batch_norm_exp.running_var torch.Size([480])\n",
      "block_list.6.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.6.depth_conv.weight torch.Size([480, 1, 3, 3])\n",
      "block_list.6.batch_norm_depth.weight torch.Size([480])\n",
      "block_list.6.batch_norm_depth.bias torch.Size([480])\n",
      "block_list.6.batch_norm_depth.running_mean torch.Size([480])\n",
      "block_list.6.batch_norm_depth.running_var torch.Size([480])\n",
      "block_list.6.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.6.conv_se_reduce.weight torch.Size([20, 480, 1, 1])\n",
      "block_list.6.conv_se_reduce.bias torch.Size([20])\n",
      "block_list.6.conv_se_expand.weight torch.Size([480, 20, 1, 1])\n",
      "block_list.6.conv_se_expand.bias torch.Size([480])\n",
      "block_list.6.conv_output.weight torch.Size([80, 480, 1, 1])\n",
      "block_list.6.batch_norm_last.weight torch.Size([80])\n",
      "block_list.6.batch_norm_last.bias torch.Size([80])\n",
      "block_list.6.batch_norm_last.running_mean torch.Size([80])\n",
      "block_list.6.batch_norm_last.running_var torch.Size([80])\n",
      "block_list.6.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.7.conv_expand.weight torch.Size([480, 80, 1, 1])\n",
      "block_list.7.batch_norm_exp.weight torch.Size([480])\n",
      "block_list.7.batch_norm_exp.bias torch.Size([480])\n",
      "block_list.7.batch_norm_exp.running_mean torch.Size([480])\n",
      "block_list.7.batch_norm_exp.running_var torch.Size([480])\n",
      "block_list.7.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.7.depth_conv.weight torch.Size([480, 1, 3, 3])\n",
      "block_list.7.batch_norm_depth.weight torch.Size([480])\n",
      "block_list.7.batch_norm_depth.bias torch.Size([480])\n",
      "block_list.7.batch_norm_depth.running_mean torch.Size([480])\n",
      "block_list.7.batch_norm_depth.running_var torch.Size([480])\n",
      "block_list.7.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.7.conv_se_reduce.weight torch.Size([20, 480, 1, 1])\n",
      "block_list.7.conv_se_reduce.bias torch.Size([20])\n",
      "block_list.7.conv_se_expand.weight torch.Size([480, 20, 1, 1])\n",
      "block_list.7.conv_se_expand.bias torch.Size([480])\n",
      "block_list.7.conv_output.weight torch.Size([80, 480, 1, 1])\n",
      "block_list.7.batch_norm_last.weight torch.Size([80])\n",
      "block_list.7.batch_norm_last.bias torch.Size([80])\n",
      "block_list.7.batch_norm_last.running_mean torch.Size([80])\n",
      "block_list.7.batch_norm_last.running_var torch.Size([80])\n",
      "block_list.7.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.8.conv_expand.weight torch.Size([480, 80, 1, 1])\n",
      "block_list.8.batch_norm_exp.weight torch.Size([480])\n",
      "block_list.8.batch_norm_exp.bias torch.Size([480])\n",
      "block_list.8.batch_norm_exp.running_mean torch.Size([480])\n",
      "block_list.8.batch_norm_exp.running_var torch.Size([480])\n",
      "block_list.8.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.8.depth_conv.weight torch.Size([480, 1, 5, 5])\n",
      "block_list.8.batch_norm_depth.weight torch.Size([480])\n",
      "block_list.8.batch_norm_depth.bias torch.Size([480])\n",
      "block_list.8.batch_norm_depth.running_mean torch.Size([480])\n",
      "block_list.8.batch_norm_depth.running_var torch.Size([480])\n",
      "block_list.8.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.8.conv_se_reduce.weight torch.Size([20, 480, 1, 1])\n",
      "block_list.8.conv_se_reduce.bias torch.Size([20])\n",
      "block_list.8.conv_se_expand.weight torch.Size([480, 20, 1, 1])\n",
      "block_list.8.conv_se_expand.bias torch.Size([480])\n",
      "block_list.8.conv_output.weight torch.Size([112, 480, 1, 1])\n",
      "block_list.8.batch_norm_last.weight torch.Size([112])\n",
      "block_list.8.batch_norm_last.bias torch.Size([112])\n",
      "block_list.8.batch_norm_last.running_mean torch.Size([112])\n",
      "block_list.8.batch_norm_last.running_var torch.Size([112])\n",
      "block_list.8.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.9.conv_expand.weight torch.Size([672, 112, 1, 1])\n",
      "block_list.9.batch_norm_exp.weight torch.Size([672])\n",
      "block_list.9.batch_norm_exp.bias torch.Size([672])\n",
      "block_list.9.batch_norm_exp.running_mean torch.Size([672])\n",
      "block_list.9.batch_norm_exp.running_var torch.Size([672])\n",
      "block_list.9.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.9.depth_conv.weight torch.Size([672, 1, 5, 5])\n",
      "block_list.9.batch_norm_depth.weight torch.Size([672])\n",
      "block_list.9.batch_norm_depth.bias torch.Size([672])\n",
      "block_list.9.batch_norm_depth.running_mean torch.Size([672])\n",
      "block_list.9.batch_norm_depth.running_var torch.Size([672])\n",
      "block_list.9.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.9.conv_se_reduce.weight torch.Size([28, 672, 1, 1])\n",
      "block_list.9.conv_se_reduce.bias torch.Size([28])\n",
      "block_list.9.conv_se_expand.weight torch.Size([672, 28, 1, 1])\n",
      "block_list.9.conv_se_expand.bias torch.Size([672])\n",
      "block_list.9.conv_output.weight torch.Size([112, 672, 1, 1])\n",
      "block_list.9.batch_norm_last.weight torch.Size([112])\n",
      "block_list.9.batch_norm_last.bias torch.Size([112])\n",
      "block_list.9.batch_norm_last.running_mean torch.Size([112])\n",
      "block_list.9.batch_norm_last.running_var torch.Size([112])\n",
      "block_list.9.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.10.conv_expand.weight torch.Size([672, 112, 1, 1])\n",
      "block_list.10.batch_norm_exp.weight torch.Size([672])\n",
      "block_list.10.batch_norm_exp.bias torch.Size([672])\n",
      "block_list.10.batch_norm_exp.running_mean torch.Size([672])\n",
      "block_list.10.batch_norm_exp.running_var torch.Size([672])\n",
      "block_list.10.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.10.depth_conv.weight torch.Size([672, 1, 5, 5])\n",
      "block_list.10.batch_norm_depth.weight torch.Size([672])\n",
      "block_list.10.batch_norm_depth.bias torch.Size([672])\n",
      "block_list.10.batch_norm_depth.running_mean torch.Size([672])\n",
      "block_list.10.batch_norm_depth.running_var torch.Size([672])\n",
      "block_list.10.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.10.conv_se_reduce.weight torch.Size([28, 672, 1, 1])\n",
      "block_list.10.conv_se_reduce.bias torch.Size([28])\n",
      "block_list.10.conv_se_expand.weight torch.Size([672, 28, 1, 1])\n",
      "block_list.10.conv_se_expand.bias torch.Size([672])\n",
      "block_list.10.conv_output.weight torch.Size([112, 672, 1, 1])\n",
      "block_list.10.batch_norm_last.weight torch.Size([112])\n",
      "block_list.10.batch_norm_last.bias torch.Size([112])\n",
      "block_list.10.batch_norm_last.running_mean torch.Size([112])\n",
      "block_list.10.batch_norm_last.running_var torch.Size([112])\n",
      "block_list.10.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.11.conv_expand.weight torch.Size([672, 112, 1, 1])\n",
      "block_list.11.batch_norm_exp.weight torch.Size([672])\n",
      "block_list.11.batch_norm_exp.bias torch.Size([672])\n",
      "block_list.11.batch_norm_exp.running_mean torch.Size([672])\n",
      "block_list.11.batch_norm_exp.running_var torch.Size([672])\n",
      "block_list.11.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.11.depth_conv.weight torch.Size([672, 1, 5, 5])\n",
      "block_list.11.batch_norm_depth.weight torch.Size([672])\n",
      "block_list.11.batch_norm_depth.bias torch.Size([672])\n",
      "block_list.11.batch_norm_depth.running_mean torch.Size([672])\n",
      "block_list.11.batch_norm_depth.running_var torch.Size([672])\n",
      "block_list.11.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.11.conv_se_reduce.weight torch.Size([28, 672, 1, 1])\n",
      "block_list.11.conv_se_reduce.bias torch.Size([28])\n",
      "block_list.11.conv_se_expand.weight torch.Size([672, 28, 1, 1])\n",
      "block_list.11.conv_se_expand.bias torch.Size([672])\n",
      "block_list.11.conv_output.weight torch.Size([192, 672, 1, 1])\n",
      "block_list.11.batch_norm_last.weight torch.Size([192])\n",
      "block_list.11.batch_norm_last.bias torch.Size([192])\n",
      "block_list.11.batch_norm_last.running_mean torch.Size([192])\n",
      "block_list.11.batch_norm_last.running_var torch.Size([192])\n",
      "block_list.11.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.12.conv_expand.weight torch.Size([1152, 192, 1, 1])\n",
      "block_list.12.batch_norm_exp.weight torch.Size([1152])\n",
      "block_list.12.batch_norm_exp.bias torch.Size([1152])\n",
      "block_list.12.batch_norm_exp.running_mean torch.Size([1152])\n",
      "block_list.12.batch_norm_exp.running_var torch.Size([1152])\n",
      "block_list.12.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.12.depth_conv.weight torch.Size([1152, 1, 5, 5])\n",
      "block_list.12.batch_norm_depth.weight torch.Size([1152])\n",
      "block_list.12.batch_norm_depth.bias torch.Size([1152])\n",
      "block_list.12.batch_norm_depth.running_mean torch.Size([1152])\n",
      "block_list.12.batch_norm_depth.running_var torch.Size([1152])\n",
      "block_list.12.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.12.conv_se_reduce.weight torch.Size([48, 1152, 1, 1])\n",
      "block_list.12.conv_se_reduce.bias torch.Size([48])\n",
      "block_list.12.conv_se_expand.weight torch.Size([1152, 48, 1, 1])\n",
      "block_list.12.conv_se_expand.bias torch.Size([1152])\n",
      "block_list.12.conv_output.weight torch.Size([192, 1152, 1, 1])\n",
      "block_list.12.batch_norm_last.weight torch.Size([192])\n",
      "block_list.12.batch_norm_last.bias torch.Size([192])\n",
      "block_list.12.batch_norm_last.running_mean torch.Size([192])\n",
      "block_list.12.batch_norm_last.running_var torch.Size([192])\n",
      "block_list.12.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.13.conv_expand.weight torch.Size([1152, 192, 1, 1])\n",
      "block_list.13.batch_norm_exp.weight torch.Size([1152])\n",
      "block_list.13.batch_norm_exp.bias torch.Size([1152])\n",
      "block_list.13.batch_norm_exp.running_mean torch.Size([1152])\n",
      "block_list.13.batch_norm_exp.running_var torch.Size([1152])\n",
      "block_list.13.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.13.depth_conv.weight torch.Size([1152, 1, 5, 5])\n",
      "block_list.13.batch_norm_depth.weight torch.Size([1152])\n",
      "block_list.13.batch_norm_depth.bias torch.Size([1152])\n",
      "block_list.13.batch_norm_depth.running_mean torch.Size([1152])\n",
      "block_list.13.batch_norm_depth.running_var torch.Size([1152])\n",
      "block_list.13.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.13.conv_se_reduce.weight torch.Size([48, 1152, 1, 1])\n",
      "block_list.13.conv_se_reduce.bias torch.Size([48])\n",
      "block_list.13.conv_se_expand.weight torch.Size([1152, 48, 1, 1])\n",
      "block_list.13.conv_se_expand.bias torch.Size([1152])\n",
      "block_list.13.conv_output.weight torch.Size([192, 1152, 1, 1])\n",
      "block_list.13.batch_norm_last.weight torch.Size([192])\n",
      "block_list.13.batch_norm_last.bias torch.Size([192])\n",
      "block_list.13.batch_norm_last.running_mean torch.Size([192])\n",
      "block_list.13.batch_norm_last.running_var torch.Size([192])\n",
      "block_list.13.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.14.conv_expand.weight torch.Size([1152, 192, 1, 1])\n",
      "block_list.14.batch_norm_exp.weight torch.Size([1152])\n",
      "block_list.14.batch_norm_exp.bias torch.Size([1152])\n",
      "block_list.14.batch_norm_exp.running_mean torch.Size([1152])\n",
      "block_list.14.batch_norm_exp.running_var torch.Size([1152])\n",
      "block_list.14.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.14.depth_conv.weight torch.Size([1152, 1, 5, 5])\n",
      "block_list.14.batch_norm_depth.weight torch.Size([1152])\n",
      "block_list.14.batch_norm_depth.bias torch.Size([1152])\n",
      "block_list.14.batch_norm_depth.running_mean torch.Size([1152])\n",
      "block_list.14.batch_norm_depth.running_var torch.Size([1152])\n",
      "block_list.14.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.14.conv_se_reduce.weight torch.Size([48, 1152, 1, 1])\n",
      "block_list.14.conv_se_reduce.bias torch.Size([48])\n",
      "block_list.14.conv_se_expand.weight torch.Size([1152, 48, 1, 1])\n",
      "block_list.14.conv_se_expand.bias torch.Size([1152])\n",
      "block_list.14.conv_output.weight torch.Size([192, 1152, 1, 1])\n",
      "block_list.14.batch_norm_last.weight torch.Size([192])\n",
      "block_list.14.batch_norm_last.bias torch.Size([192])\n",
      "block_list.14.batch_norm_last.running_mean torch.Size([192])\n",
      "block_list.14.batch_norm_last.running_var torch.Size([192])\n",
      "block_list.14.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.15.conv_expand.weight torch.Size([1152, 192, 1, 1])\n",
      "block_list.15.batch_norm_exp.weight torch.Size([1152])\n",
      "block_list.15.batch_norm_exp.bias torch.Size([1152])\n",
      "block_list.15.batch_norm_exp.running_mean torch.Size([1152])\n",
      "block_list.15.batch_norm_exp.running_var torch.Size([1152])\n",
      "block_list.15.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.15.depth_conv.weight torch.Size([1152, 1, 3, 3])\n",
      "block_list.15.batch_norm_depth.weight torch.Size([1152])\n",
      "block_list.15.batch_norm_depth.bias torch.Size([1152])\n",
      "block_list.15.batch_norm_depth.running_mean torch.Size([1152])\n",
      "block_list.15.batch_norm_depth.running_var torch.Size([1152])\n",
      "block_list.15.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.15.conv_se_reduce.weight torch.Size([48, 1152, 1, 1])\n",
      "block_list.15.conv_se_reduce.bias torch.Size([48])\n",
      "block_list.15.conv_se_expand.weight torch.Size([1152, 48, 1, 1])\n",
      "block_list.15.conv_se_expand.bias torch.Size([1152])\n",
      "block_list.15.conv_output.weight torch.Size([320, 1152, 1, 1])\n",
      "block_list.15.batch_norm_last.weight torch.Size([320])\n",
      "block_list.15.batch_norm_last.bias torch.Size([320])\n",
      "block_list.15.batch_norm_last.running_mean torch.Size([320])\n",
      "block_list.15.batch_norm_last.running_var torch.Size([320])\n",
      "block_list.15.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "conv_final.weight torch.Size([1280, 320, 1, 1])\n",
      "batch_norm_final.weight torch.Size([1280])\n",
      "batch_norm_final.bias torch.Size([1280])\n",
      "batch_norm_final.running_mean torch.Size([1280])\n",
      "batch_norm_final.running_var torch.Size([1280])\n",
      "batch_norm_final.num_batches_tracked torch.Size([])\n",
      "linear.weight torch.Size([1000, 1280])\n",
      "linear.bias torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "for k,v in effnet.state_dict().items():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8ea30785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the state dictionary from the pytorch model to be editted\n",
    "sd = effnet.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e8c66005",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_pytorch_convert = {'se_reduce':'conv_se_reduce','se_expand':'conv_se_expand','project_conv':'conv_output',\n",
    "                       'expand_conv':'conv_expand', 'dwconv':'depth_conv','top_conv':'conv_final','probs':'linear',\n",
    "                      'stem_conv':'conv1', 'stem_bn':'batch_norm1', 'top_bn':'batch_norm_final','expand_bn':'batch_norm_exp',\n",
    "                        'bn':'batch_norm_depth','project_bn':'batch_norm_last'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9c1e10f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesnt work on depth conv\n",
    "# transpose works on everything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1eeb2e04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "stem_conv\n",
      "==========================================\n",
      "stem_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block1a_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block1a_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block1a_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block1a_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block1a_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block1a_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2a_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2a_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2a_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2a_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2a_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2a_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2a_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2a_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2b_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2b_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2b_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2b_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2b_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2b_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2b_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2b_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3a_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3a_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3a_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3a_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3a_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3a_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3a_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3a_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3b_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3b_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3b_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3b_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3b_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3b_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3b_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3b_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4a_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4a_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4a_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4a_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4a_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4a_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4a_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4a_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4b_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4b_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4b_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4b_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4b_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4b_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4b_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4b_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4c_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4c_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4c_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4c_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4c_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4c_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4c_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4c_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5a_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5a_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5a_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5a_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5a_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5a_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5a_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5a_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5b_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5b_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5b_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5b_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5b_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5b_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5b_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5b_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5c_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5c_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5c_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5c_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5c_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5c_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5c_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5c_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6a_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6a_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6a_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6a_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6a_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6a_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6a_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6a_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6b_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6b_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6b_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6b_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6b_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6b_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6b_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6b_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6c_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6c_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6c_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6c_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6c_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6c_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6c_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6c_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6d_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6d_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6d_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6d_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6d_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6d_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6d_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6d_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block7a_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block7a_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block7a_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block7a_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block7a_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block7a_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block7a_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block7a_project_bn\n",
      "==========================================\n",
      "top_conv\n",
      "==========================================\n",
      "top_bn\n",
      "==========================================\n",
      "probs\n"
     ]
    }
   ],
   "source": [
    "layer = -1\n",
    "name = ''\n",
    "for i,l in enumerate(model.layers):\n",
    "    if l.get_weights():\n",
    "        org_name = l.get_config()['name']\n",
    "        weights = l.get_weights()\n",
    "        # any weights in a block unit\n",
    "        if org_name[0:5] == 'block':\n",
    "            if org_name[5:7] != name:\n",
    "                layer += 1\n",
    "                name=org_name[5:7]\n",
    "            base_pyt = 'block_list.'\n",
    "            print('======'*7)\n",
    "            print('keras')\n",
    "            print('pytorch')\n",
    "            print(org_name)\n",
    "            if keras_pytorch_convert.get(org_name[8:]):\n",
    "                use = keras_pytorch_convert[org_name[8:]]\n",
    "                py_name = f'{base_pyt}{layer}.{use}'\n",
    "                if org_name[8:] == 'dwconv':\n",
    "                    new_weights = torch.from_numpy(weights[0]).permute(2,3,0,1)\n",
    "                    sd[py_name+'.weight'] = new_weights\n",
    "                # any non batchnorm or non depthconv layer\n",
    "                elif 'bn' not in org_name:\n",
    "                    new_weights = weights[0]\n",
    "                    if 'conv' in org_name:\n",
    "                        new_weights = torch.from_numpy(np.moveaxis(new_weights, [-1, -2], [0, 1]))\n",
    "                    else:\n",
    "                        new_weights = torch.from_numpy(new_weights.T)\n",
    "                    sd[py_name+'.weight'] = new_weights\n",
    "                    # this means theres a bias to account for\n",
    "                    if len(weights)>1:\n",
    "                        new_weights = weights[1]\n",
    "                        new_weights = torch.from_numpy(new_weights)\n",
    "                        sd[py_name+'.bias'] = new_weights\n",
    "                # its a batchnorm layer\n",
    "                else:\n",
    "                    new_weights = weights[0]\n",
    "                    new_weights = torch.from_numpy(new_weights)\n",
    "                    sd[py_name+'.weight'] = new_weights\n",
    "                    # this means theres a bias to account for\n",
    "                    if len(weights)>1:\n",
    "                        new_weights = [torch.from_numpy(i) for i in weights]\n",
    "                        sd[py_name+'.bias'] = new_weights[1]\n",
    "                        sd[py_name+'.running_mean'] = new_weights[2]\n",
    "                        sd[py_name+'.running_var'] = new_weights[3]\n",
    "            else:\n",
    "                continue\n",
    "        # initial convs or final convs\n",
    "        else:\n",
    "            print('======'*7)\n",
    "            print(org_name)\n",
    "            py_name = keras_pytorch_convert[org_name]\n",
    "            if 'bn' in org_name:\n",
    "                new_weights = torch.from_numpy(weights[0])\n",
    "            else:\n",
    "                new_weights = torch.from_numpy(np.moveaxis(weights[0], [-1, -2], [0, 1]))\n",
    "            sd[py_name+'.weight'] = new_weights\n",
    "            # handle bias weights as well\n",
    "            if len(weights) > 1:\n",
    "                new_weights = [torch.from_numpy(i) for i in weights]\n",
    "                sd[py_name+'.bias'] = new_weights[1]\n",
    "                if org_name == 'probs':\n",
    "                    continue\n",
    "                sd[py_name+'.running_mean'] = new_weights[2]\n",
    "                sd[py_name+'.running_var'] = new_weights[3]\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e9bd5a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effnet.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a0c588c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3) torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(224,224,3)\n",
    "# x = np.ones((224,224,3))\n",
    "x = np.array([x])\n",
    "x_t = torch.from_numpy(x)\n",
    "x_t = x_t.permute(0,3,1,2).float()\n",
    "print(x.shape, x_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "24a40b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = keras.Model(inputs=model.inputs,\n",
    "                        outputs={layer.get_config()['name']:layer.output for layer in model.layers})\n",
    "features = extractor.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "49efa147",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (method, Parameter, NoneType, tuple, str, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mmethod\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mNoneType\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mstr\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mmethod\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mNoneType\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mstr\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#pyt version\u001b[39;00m\n\u001b[0;32m      2\u001b[0m effnet\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m----> 3\u001b[0m y_t \u001b[38;5;241m=\u001b[39m \u001b[43meffnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_effnet\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\UROP\\efficient_net\\pytorch-efficientnet\\main.py:213\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;66;03m# go through all the conv blocks\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, block_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_list):\n\u001b[1;32m--> 213\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock_i\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m    216\u001b[0m \u001b[38;5;66;03m# final conv layer\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_effnet\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\UROP\\efficient_net\\pytorch-efficientnet\\main.py:289\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    287\u001b[0m     padding \u001b[38;5;241m=\u001b[39m correct_pad(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size)\n\u001b[0;32m    288\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mx, pad\u001b[38;5;241m=\u001b[39mpadding, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 289\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdepth_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth_conv\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    291\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_norm_depth(x)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_effnet\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_effnet\\lib\\site-packages\\torch\\nn\\modules\\conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_effnet\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    441\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    442\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (method, Parameter, NoneType, tuple, str, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mmethod\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mNoneType\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mstr\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mmethod\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mNoneType\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mstr\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "#pyt version\n",
    "effnet.eval()\n",
    "y_t = effnet(x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aa360d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['conv1', 'bn1', 'activation1'])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effnet.outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "01a3889d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_1',\n",
       " 'stem_conv',\n",
       " 'stem_bn',\n",
       " 'stem_activation',\n",
       " 'block1a_dwconv',\n",
       " 'block1a_bn',\n",
       " 'block1a_activation',\n",
       " 'block1a_se_squeeze',\n",
       " 'block1a_se_reshape',\n",
       " 'block1a_se_reduce',\n",
       " 'block1a_se_expand',\n",
       " 'block1a_se_excite',\n",
       " 'block1a_project_conv',\n",
       " 'block1a_project_bn',\n",
       " 'block2a_expand_conv',\n",
       " 'block2a_expand_bn',\n",
       " 'block2a_expand_activation',\n",
       " 'block2a_dwconv',\n",
       " 'block2a_bn',\n",
       " 'block2a_activation']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(features.keys())[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f285feda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 112, 32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_keras = features['stem_conv'][0]\n",
    "test_keras.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ffeab38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 112, 112])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_pyt = effnet.block_list[0].outputs['depth_conv'][0].permute(1,2,0)[0,0:5,0:5]\n",
    "test_pyt = effnet.outputs['conv1'][0]\n",
    "test_pyt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3d30ff9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.89257056,  0.64466476, -0.07190483,  0.25204936,  2.086779  ],\n",
       "       [ 1.8468978 , -1.3588157 ,  0.06381522,  1.1926026 , -0.07757401],\n",
       "       [-1.2698512 ,  0.8172054 , -0.16187674, -0.49342743, -0.05741452],\n",
       "       [ 0.6794041 , -3.6215224 , -0.03492995,  1.2019897 , -1.0170628 ],\n",
       "       [ 0.3571217 , -0.31503797,  0.28771666,  0.64856356,  0.06055938]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_keras[0:5,0:5,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2e107fc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0035,  0.0025, -0.0003,  0.0010,  0.0082],\n",
       "        [ 0.0072, -0.0053,  0.0003,  0.0047, -0.0003],\n",
       "        [-0.0050,  0.0032, -0.0006, -0.0019, -0.0002],\n",
       "        [ 0.0027, -0.0142, -0.0001,  0.0047, -0.0040],\n",
       "        [ 0.0014, -0.0012,  0.0011,  0.0025,  0.0002]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pyt.permute(1,2,0)[0:5,0:5,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8cdee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1[0:5,0:5,1], x_t1.permute(1,2,0)[0:5,0:5,1],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1417fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((0, 1), (0, 1)), (0, 1, 0, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (top, bottom) (left, right)\n",
    "imagenet_utils.correct_pad(x,3), correct_pad([1, 3, 224, 224],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f6e721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c67943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_pad(input_shape, kernel_size):\n",
    "    \"\"\"Returns a tuple for zero-padding for 2D convolution with downsampling.\n",
    "    Args:\n",
    "      input_shape: Input img shape\n",
    "      kernel_size: An integer or tuple/list of 2 integers.\n",
    "    Returns:\n",
    "      A tuple.\n",
    "    \"\"\"\n",
    "    img_dim = 2\n",
    "    input_size = list(input_shape)[img_dim: (img_dim + 2)]\n",
    "    # 128, 128\n",
    "    if type(kernel_size):\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    # 3, 3\n",
    "    if input_size[0] is None:\n",
    "        adjust = (1, 1)\n",
    "    else:\n",
    "        adjust = (1 - input_size[0] % 2, 1 - input_size[1] % 2)\n",
    "    correct = (kernel_size[0] // 2, kernel_size[1] // 2)\n",
    "    return (\n",
    "        correct[1] - adjust[1], correct[1],\n",
    "        correct[0] - adjust[0], correct[0],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "847914f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3) torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(224,224,3)\n",
    "# x = np.ones((224,224,3))\n",
    "x = np.array([x])\n",
    "x_t = torch.from_numpy(x)\n",
    "x_t = x_t.permute(0,3,1,2).float()\n",
    "print(x.shape, x_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "91ada641",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = layers.Rescaling(1.0 / 255.0)(x)\n",
    "x1 = layers.Normalization(axis=1)(x1)\n",
    "a,b,c,d = correct_pad(x1.shape, 3)\n",
    "# (top, bottom) (left, right) for padding argument\n",
    "x1 = layers.ZeroPadding2D(\n",
    "        padding=imagenet_utils.correct_pad(x,3), data_format='channels_last',name=\"stem_conv_pad\")(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4d29ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale = lambda x: x * 1./255\n",
    "pyt_transforms = transforms.Compose([\n",
    "            transforms.Lambda(rescale),\n",
    "            transforms.Normalize(0, 1),\n",
    "            # padding left, right, top, bottom\n",
    "            transforms.Lambda(nn.ZeroPad2d(padding=correct_pad([1, 3, 224, 224],3))),]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "852143ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t1 = pyt_transforms(x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "363fe5d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 225, 225, 3]), torch.Size([1, 3, 225, 225]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape, x_t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b1e1bd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       " array([[0.00390635, 0.00348694, 0.00117885],\n",
       "        [0.00331971, 0.00014537, 0.00239421],\n",
       "        [0.00261566, 0.00023715, 0.00024944]], dtype=float32)>,\n",
       " tensor([[0.0039, 0.0035, 0.0012],\n",
       "         [0.0033, 0.0001, 0.0024],\n",
       "         [0.0026, 0.0002, 0.0002]]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1[0][0:3,0:3,0], x_t1[0].permute(1,2,0)[0:3,0:3,0],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "693f63d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras weights:  (3, 3, 3, 32)\n",
      "torch weights:  torch.Size([32, 3, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.layers[1].get_weights()[0]\n",
    "print('keras weights: ',weights.shape)\n",
    "keras_conv = layers.Conv2D(32, 3, strides=2, padding=\"valid\", use_bias=False, weights=np.array([weights]),data_format=\"channels_last\")\n",
    "pyt_conv = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=\"valid\", bias=False)\n",
    "sd = pyt_conv.state_dict()\n",
    "weights_t = torch.from_numpy(np.moveaxis(weights, [-1, -2], [0, 1]))\n",
    "# weights_t = torch.from_numpy(weights.T)\n",
    "print('torch weights: ',weights_t.shape)\n",
    "sd['weight']= weights_t \n",
    "pyt_conv.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3aa993fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = keras_conv(x1)[0]\n",
    "x_t1 = pyt_conv(x_t1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7d025476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([112, 112, 32]), torch.Size([32, 112, 112]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape, x_t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "40a296a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       " array([[ 5.5416930e-03, -7.9710241e-03, -2.9920922e-03,  4.8286426e-03,\n",
       "         -5.8296393e-03],\n",
       "        [ 1.5536631e-03, -6.1574881e-03, -2.5852348e-03, -4.3254676e-03,\n",
       "         -5.1139481e-04],\n",
       "        [-1.7202171e-03, -2.9728068e-03, -4.9127722e-03, -3.0871422e-05,\n",
       "          2.7573048e-03],\n",
       "        [-6.9631397e-04,  1.1423291e-03,  8.1694190e-05, -1.0351182e-03,\n",
       "          6.3428539e-04],\n",
       "        [ 2.2146062e-03,  4.2086621e-03,  3.3108596e-04, -4.8314990e-03,\n",
       "          3.0117733e-03]], dtype=float32)>,\n",
       " tensor([[ 5.5417e-03, -7.9710e-03, -2.9921e-03,  4.8286e-03, -5.8296e-03],\n",
       "         [ 1.5537e-03, -6.1575e-03, -2.5852e-03, -4.3255e-03, -5.1140e-04],\n",
       "         [-1.7202e-03, -2.9728e-03, -4.9128e-03, -3.0871e-05,  2.7573e-03],\n",
       "         [-6.9631e-04,  1.1423e-03,  8.1694e-05, -1.0351e-03,  6.3429e-04],\n",
       "         [ 2.2146e-03,  4.2087e-03,  3.3109e-04, -4.8315e-03,  3.0118e-03]],\n",
       "        grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1[0:5,0:5,1], x_t1.permute(1,2,0)[0:5,0:5,1],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeca0ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f4311c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
