{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec4a3089",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import h5py\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from main import EfficientNetB0\n",
    "import efficientnet.tfkeras as efn\n",
    "from keras import layers\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from keras.applications import imagenet_utils\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7bdc2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.python.ops.numpy_ops.np_config as np_config\n",
    "np_config.enable_numpy_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "386ca38f",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "deep_prof_model = tf.keras.models.load_model('model', compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f238c206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 stem_conv (3, 3, 5, 32)\n",
      "1\n",
      "2 stem_bn (32,)\n",
      "4\n",
      "(32,)\n",
      "4 block1a_dwconv (3, 3, 32, 1)\n",
      "1\n",
      "5 block1a_bn (32,)\n",
      "4\n",
      "(32,)\n",
      "9 block1a_se_reduce (1, 1, 32, 8)\n",
      "2\n",
      "(8,)\n",
      "10 block1a_se_expand (1, 1, 8, 32)\n",
      "2\n",
      "(32,)\n",
      "12 block1a_project_conv (1, 1, 32, 16)\n",
      "1\n",
      "13 block1a_project_bn (16,)\n",
      "4\n",
      "(16,)\n",
      "14 block2a_expand_conv (1, 1, 16, 96)\n",
      "1\n",
      "15 block2a_expand_bn (96,)\n",
      "4\n",
      "(96,)\n",
      "17 block2a_dwconv (3, 3, 96, 1)\n",
      "1\n",
      "18 block2a_bn (96,)\n",
      "4\n",
      "(96,)\n",
      "22 block2a_se_reduce (1, 1, 96, 4)\n",
      "2\n",
      "(4,)\n",
      "23 block2a_se_expand (1, 1, 4, 96)\n",
      "2\n",
      "(96,)\n",
      "25 block2a_project_conv (1, 1, 96, 24)\n",
      "1\n",
      "26 block2a_project_bn (24,)\n",
      "4\n",
      "(24,)\n",
      "27 block2b_expand_conv (1, 1, 24, 144)\n",
      "1\n",
      "28 block2b_expand_bn (144,)\n",
      "4\n",
      "(144,)\n",
      "30 block2b_dwconv (3, 3, 144, 1)\n",
      "1\n",
      "31 block2b_bn (144,)\n",
      "4\n",
      "(144,)\n",
      "35 block2b_se_reduce (1, 1, 144, 6)\n",
      "2\n",
      "(6,)\n",
      "36 block2b_se_expand (1, 1, 6, 144)\n",
      "2\n",
      "(144,)\n",
      "38 block2b_project_conv (1, 1, 144, 24)\n",
      "1\n",
      "39 block2b_project_bn (24,)\n",
      "4\n",
      "(24,)\n",
      "42 block3a_expand_conv (1, 1, 24, 144)\n",
      "1\n",
      "43 block3a_expand_bn (144,)\n",
      "4\n",
      "(144,)\n",
      "45 block3a_dwconv (5, 5, 144, 1)\n",
      "1\n",
      "46 block3a_bn (144,)\n",
      "4\n",
      "(144,)\n",
      "50 block3a_se_reduce (1, 1, 144, 6)\n",
      "2\n",
      "(6,)\n",
      "51 block3a_se_expand (1, 1, 6, 144)\n",
      "2\n",
      "(144,)\n",
      "53 block3a_project_conv (1, 1, 144, 40)\n",
      "1\n",
      "54 block3a_project_bn (40,)\n",
      "4\n",
      "(40,)\n",
      "55 block3b_expand_conv (1, 1, 40, 240)\n",
      "1\n",
      "56 block3b_expand_bn (240,)\n",
      "4\n",
      "(240,)\n",
      "58 block3b_dwconv (5, 5, 240, 1)\n",
      "1\n",
      "59 block3b_bn (240,)\n",
      "4\n",
      "(240,)\n",
      "63 block3b_se_reduce (1, 1, 240, 10)\n",
      "2\n",
      "(10,)\n",
      "64 block3b_se_expand (1, 1, 10, 240)\n",
      "2\n",
      "(240,)\n",
      "66 block3b_project_conv (1, 1, 240, 40)\n",
      "1\n",
      "67 block3b_project_bn (40,)\n",
      "4\n",
      "(40,)\n",
      "70 block4a_expand_conv (1, 1, 40, 240)\n",
      "1\n",
      "71 block4a_expand_bn (240,)\n",
      "4\n",
      "(240,)\n",
      "73 block4a_dwconv (3, 3, 240, 1)\n",
      "1\n",
      "74 block4a_bn (240,)\n",
      "4\n",
      "(240,)\n",
      "78 block4a_se_reduce (1, 1, 240, 10)\n",
      "2\n",
      "(10,)\n",
      "79 block4a_se_expand (1, 1, 10, 240)\n",
      "2\n",
      "(240,)\n",
      "81 block4a_project_conv (1, 1, 240, 80)\n",
      "1\n",
      "82 block4a_project_bn (80,)\n",
      "4\n",
      "(80,)\n",
      "83 block4b_expand_conv (1, 1, 80, 480)\n",
      "1\n",
      "84 block4b_expand_bn (480,)\n",
      "4\n",
      "(480,)\n",
      "86 block4b_dwconv (3, 3, 480, 1)\n",
      "1\n",
      "87 block4b_bn (480,)\n",
      "4\n",
      "(480,)\n",
      "91 block4b_se_reduce (1, 1, 480, 20)\n",
      "2\n",
      "(20,)\n",
      "92 block4b_se_expand (1, 1, 20, 480)\n",
      "2\n",
      "(480,)\n",
      "94 block4b_project_conv (1, 1, 480, 80)\n",
      "1\n",
      "95 block4b_project_bn (80,)\n",
      "4\n",
      "(80,)\n",
      "98 block4c_expand_conv (1, 1, 80, 480)\n",
      "1\n",
      "99 block4c_expand_bn (480,)\n",
      "4\n",
      "(480,)\n",
      "101 block4c_dwconv (3, 3, 480, 1)\n",
      "1\n",
      "102 block4c_bn (480,)\n",
      "4\n",
      "(480,)\n",
      "106 block4c_se_reduce (1, 1, 480, 20)\n",
      "2\n",
      "(20,)\n",
      "107 block4c_se_expand (1, 1, 20, 480)\n",
      "2\n",
      "(480,)\n",
      "109 block4c_project_conv (1, 1, 480, 80)\n",
      "1\n",
      "110 block4c_project_bn (80,)\n",
      "4\n",
      "(80,)\n",
      "113 block5a_expand_conv (1, 1, 80, 480)\n",
      "1\n",
      "114 block5a_expand_bn (480,)\n",
      "4\n",
      "(480,)\n",
      "116 block5a_dwconv (5, 5, 480, 1)\n",
      "1\n",
      "117 block5a_bn (480,)\n",
      "4\n",
      "(480,)\n",
      "121 block5a_se_reduce (1, 1, 480, 20)\n",
      "2\n",
      "(20,)\n",
      "122 block5a_se_expand (1, 1, 20, 480)\n",
      "2\n",
      "(480,)\n",
      "124 block5a_project_conv (1, 1, 480, 112)\n",
      "1\n",
      "125 block5a_project_bn (112,)\n",
      "4\n",
      "(112,)\n",
      "126 block5b_expand_conv (1, 1, 112, 672)\n",
      "1\n",
      "127 block5b_expand_bn (672,)\n",
      "4\n",
      "(672,)\n",
      "129 block5b_dwconv (5, 5, 672, 1)\n",
      "1\n",
      "130 block5b_bn (672,)\n",
      "4\n",
      "(672,)\n",
      "134 block5b_se_reduce (1, 1, 672, 28)\n",
      "2\n",
      "(28,)\n",
      "135 block5b_se_expand (1, 1, 28, 672)\n",
      "2\n",
      "(672,)\n",
      "137 block5b_project_conv (1, 1, 672, 112)\n",
      "1\n",
      "138 block5b_project_bn (112,)\n",
      "4\n",
      "(112,)\n",
      "141 block5c_expand_conv (1, 1, 112, 672)\n",
      "1\n",
      "142 block5c_expand_bn (672,)\n",
      "4\n",
      "(672,)\n",
      "144 block5c_dwconv (5, 5, 672, 1)\n",
      "1\n",
      "145 block5c_bn (672,)\n",
      "4\n",
      "(672,)\n",
      "149 block5c_se_reduce (1, 1, 672, 28)\n",
      "2\n",
      "(28,)\n",
      "150 block5c_se_expand (1, 1, 28, 672)\n",
      "2\n",
      "(672,)\n",
      "152 block5c_project_conv (1, 1, 672, 112)\n",
      "1\n",
      "153 block5c_project_bn (112,)\n",
      "4\n",
      "(112,)\n",
      "156 block6a_expand_conv (1, 1, 112, 672)\n",
      "1\n",
      "157 block6a_expand_bn (672,)\n",
      "4\n",
      "(672,)\n",
      "159 block6a_dwconv (5, 5, 672, 1)\n",
      "1\n",
      "160 block6a_bn (672,)\n",
      "4\n",
      "(672,)\n"
     ]
    }
   ],
   "source": [
    "for i,l in enumerate(deep_prof_model.layers):\n",
    "    if l.get_weights():\n",
    "        print(i, l.get_config()['name'], l.get_weights()[0].shape)\n",
    "        print(len(l.get_weights()))\n",
    "        try:\n",
    "            print(l.get_weights()[1].shape)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55346d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = efn.EfficientNetB0(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3323609",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 stem_conv (3, 3, 3, 32)\n",
      "2 stem_bn (32,)\n",
      "(32,)\n",
      "4 block1a_dwconv (3, 3, 32, 1)\n",
      "5 block1a_bn (32,)\n",
      "(32,)\n",
      "9 block1a_se_reduce (1, 1, 32, 8)\n",
      "(8,)\n",
      "10 block1a_se_expand (1, 1, 8, 32)\n",
      "(32,)\n",
      "12 block1a_project_conv (1, 1, 32, 16)\n",
      "13 block1a_project_bn (16,)\n",
      "(16,)\n",
      "14 block2a_expand_conv (1, 1, 16, 96)\n",
      "15 block2a_expand_bn (96,)\n",
      "(96,)\n",
      "17 block2a_dwconv (3, 3, 96, 1)\n",
      "18 block2a_bn (96,)\n",
      "(96,)\n",
      "22 block2a_se_reduce (1, 1, 96, 4)\n",
      "(4,)\n",
      "23 block2a_se_expand (1, 1, 4, 96)\n",
      "(96,)\n",
      "25 block2a_project_conv (1, 1, 96, 24)\n",
      "26 block2a_project_bn (24,)\n",
      "(24,)\n",
      "27 block2b_expand_conv (1, 1, 24, 144)\n",
      "28 block2b_expand_bn (144,)\n",
      "(144,)\n",
      "30 block2b_dwconv (3, 3, 144, 1)\n",
      "31 block2b_bn (144,)\n",
      "(144,)\n",
      "35 block2b_se_reduce (1, 1, 144, 6)\n",
      "(6,)\n",
      "36 block2b_se_expand (1, 1, 6, 144)\n",
      "(144,)\n",
      "38 block2b_project_conv (1, 1, 144, 24)\n",
      "39 block2b_project_bn (24,)\n",
      "(24,)\n",
      "42 block3a_expand_conv (1, 1, 24, 144)\n",
      "43 block3a_expand_bn (144,)\n",
      "(144,)\n",
      "45 block3a_dwconv (5, 5, 144, 1)\n",
      "46 block3a_bn (144,)\n",
      "(144,)\n",
      "50 block3a_se_reduce (1, 1, 144, 6)\n",
      "(6,)\n",
      "51 block3a_se_expand (1, 1, 6, 144)\n",
      "(144,)\n",
      "53 block3a_project_conv (1, 1, 144, 40)\n",
      "54 block3a_project_bn (40,)\n",
      "(40,)\n",
      "55 block3b_expand_conv (1, 1, 40, 240)\n",
      "56 block3b_expand_bn (240,)\n",
      "(240,)\n",
      "58 block3b_dwconv (5, 5, 240, 1)\n",
      "59 block3b_bn (240,)\n",
      "(240,)\n",
      "63 block3b_se_reduce (1, 1, 240, 10)\n",
      "(10,)\n",
      "64 block3b_se_expand (1, 1, 10, 240)\n",
      "(240,)\n",
      "66 block3b_project_conv (1, 1, 240, 40)\n",
      "67 block3b_project_bn (40,)\n",
      "(40,)\n",
      "70 block4a_expand_conv (1, 1, 40, 240)\n",
      "71 block4a_expand_bn (240,)\n",
      "(240,)\n",
      "73 block4a_dwconv (3, 3, 240, 1)\n",
      "74 block4a_bn (240,)\n",
      "(240,)\n",
      "78 block4a_se_reduce (1, 1, 240, 10)\n",
      "(10,)\n",
      "79 block4a_se_expand (1, 1, 10, 240)\n",
      "(240,)\n",
      "81 block4a_project_conv (1, 1, 240, 80)\n",
      "82 block4a_project_bn (80,)\n",
      "(80,)\n",
      "83 block4b_expand_conv (1, 1, 80, 480)\n",
      "84 block4b_expand_bn (480,)\n",
      "(480,)\n",
      "86 block4b_dwconv (3, 3, 480, 1)\n",
      "87 block4b_bn (480,)\n",
      "(480,)\n",
      "91 block4b_se_reduce (1, 1, 480, 20)\n",
      "(20,)\n",
      "92 block4b_se_expand (1, 1, 20, 480)\n",
      "(480,)\n",
      "94 block4b_project_conv (1, 1, 480, 80)\n",
      "95 block4b_project_bn (80,)\n",
      "(80,)\n",
      "98 block4c_expand_conv (1, 1, 80, 480)\n",
      "99 block4c_expand_bn (480,)\n",
      "(480,)\n",
      "101 block4c_dwconv (3, 3, 480, 1)\n",
      "102 block4c_bn (480,)\n",
      "(480,)\n",
      "106 block4c_se_reduce (1, 1, 480, 20)\n",
      "(20,)\n",
      "107 block4c_se_expand (1, 1, 20, 480)\n",
      "(480,)\n",
      "109 block4c_project_conv (1, 1, 480, 80)\n",
      "110 block4c_project_bn (80,)\n",
      "(80,)\n",
      "113 block5a_expand_conv (1, 1, 80, 480)\n",
      "114 block5a_expand_bn (480,)\n",
      "(480,)\n",
      "116 block5a_dwconv (5, 5, 480, 1)\n",
      "117 block5a_bn (480,)\n",
      "(480,)\n",
      "121 block5a_se_reduce (1, 1, 480, 20)\n",
      "(20,)\n",
      "122 block5a_se_expand (1, 1, 20, 480)\n",
      "(480,)\n",
      "124 block5a_project_conv (1, 1, 480, 112)\n",
      "125 block5a_project_bn (112,)\n",
      "(112,)\n",
      "126 block5b_expand_conv (1, 1, 112, 672)\n",
      "127 block5b_expand_bn (672,)\n",
      "(672,)\n",
      "129 block5b_dwconv (5, 5, 672, 1)\n",
      "130 block5b_bn (672,)\n",
      "(672,)\n",
      "134 block5b_se_reduce (1, 1, 672, 28)\n",
      "(28,)\n",
      "135 block5b_se_expand (1, 1, 28, 672)\n",
      "(672,)\n",
      "137 block5b_project_conv (1, 1, 672, 112)\n",
      "138 block5b_project_bn (112,)\n",
      "(112,)\n",
      "141 block5c_expand_conv (1, 1, 112, 672)\n",
      "142 block5c_expand_bn (672,)\n",
      "(672,)\n",
      "144 block5c_dwconv (5, 5, 672, 1)\n",
      "145 block5c_bn (672,)\n",
      "(672,)\n",
      "149 block5c_se_reduce (1, 1, 672, 28)\n",
      "(28,)\n",
      "150 block5c_se_expand (1, 1, 28, 672)\n",
      "(672,)\n",
      "152 block5c_project_conv (1, 1, 672, 112)\n",
      "153 block5c_project_bn (112,)\n",
      "(112,)\n",
      "156 block6a_expand_conv (1, 1, 112, 672)\n",
      "157 block6a_expand_bn (672,)\n",
      "(672,)\n",
      "159 block6a_dwconv (5, 5, 672, 1)\n",
      "160 block6a_bn (672,)\n",
      "(672,)\n",
      "164 block6a_se_reduce (1, 1, 672, 28)\n",
      "(28,)\n",
      "165 block6a_se_expand (1, 1, 28, 672)\n",
      "(672,)\n",
      "167 block6a_project_conv (1, 1, 672, 192)\n",
      "168 block6a_project_bn (192,)\n",
      "(192,)\n",
      "169 block6b_expand_conv (1, 1, 192, 1152)\n",
      "170 block6b_expand_bn (1152,)\n",
      "(1152,)\n",
      "172 block6b_dwconv (5, 5, 1152, 1)\n",
      "173 block6b_bn (1152,)\n",
      "(1152,)\n",
      "177 block6b_se_reduce (1, 1, 1152, 48)\n",
      "(48,)\n",
      "178 block6b_se_expand (1, 1, 48, 1152)\n",
      "(1152,)\n",
      "180 block6b_project_conv (1, 1, 1152, 192)\n",
      "181 block6b_project_bn (192,)\n",
      "(192,)\n",
      "184 block6c_expand_conv (1, 1, 192, 1152)\n",
      "185 block6c_expand_bn (1152,)\n",
      "(1152,)\n",
      "187 block6c_dwconv (5, 5, 1152, 1)\n",
      "188 block6c_bn (1152,)\n",
      "(1152,)\n",
      "192 block6c_se_reduce (1, 1, 1152, 48)\n",
      "(48,)\n",
      "193 block6c_se_expand (1, 1, 48, 1152)\n",
      "(1152,)\n",
      "195 block6c_project_conv (1, 1, 1152, 192)\n",
      "196 block6c_project_bn (192,)\n",
      "(192,)\n",
      "199 block6d_expand_conv (1, 1, 192, 1152)\n",
      "200 block6d_expand_bn (1152,)\n",
      "(1152,)\n",
      "202 block6d_dwconv (5, 5, 1152, 1)\n",
      "203 block6d_bn (1152,)\n",
      "(1152,)\n",
      "207 block6d_se_reduce (1, 1, 1152, 48)\n",
      "(48,)\n",
      "208 block6d_se_expand (1, 1, 48, 1152)\n",
      "(1152,)\n",
      "210 block6d_project_conv (1, 1, 1152, 192)\n",
      "211 block6d_project_bn (192,)\n",
      "(192,)\n",
      "214 block7a_expand_conv (1, 1, 192, 1152)\n",
      "215 block7a_expand_bn (1152,)\n",
      "(1152,)\n",
      "217 block7a_dwconv (3, 3, 1152, 1)\n",
      "218 block7a_bn (1152,)\n",
      "(1152,)\n",
      "222 block7a_se_reduce (1, 1, 1152, 48)\n",
      "(48,)\n",
      "223 block7a_se_expand (1, 1, 48, 1152)\n",
      "(1152,)\n",
      "225 block7a_project_conv (1, 1, 1152, 320)\n",
      "226 block7a_project_bn (320,)\n",
      "(320,)\n",
      "227 top_conv (1, 1, 320, 1280)\n",
      "228 top_bn (1280,)\n",
      "(1280,)\n",
      "232 probs (1280, 1000)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "for i,l in enumerate(model.layers):\n",
    "    if l.get_weights():\n",
    "        print(i, l.get_config()['name'], l.get_weights()[0].shape)\n",
    "        try:\n",
    "            print(l.get_weights()[1].shape)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b87f2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 0, 1) correct_padding\n",
      "(0, 1, 0, 1) correct_padding\n"
     ]
    }
   ],
   "source": [
    "effnet = EfficientNetB0(input_shape=[1, 3, 224, 224], num_channels=3)\n",
    "effnet1 = EfficientNetB0(input_shape=[1, 3, 224, 224],num_channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "945fd784",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([32, 3, 3, 3])\n",
      "batch_norm1.weight torch.Size([32])\n",
      "batch_norm1.bias torch.Size([32])\n",
      "batch_norm1.running_mean torch.Size([32])\n",
      "batch_norm1.running_var torch.Size([32])\n",
      "batch_norm1.num_batches_tracked torch.Size([])\n",
      "block_list.0.depth_conv.weight torch.Size([32, 1, 3, 3])\n",
      "block_list.0.batch_norm_depth.weight torch.Size([32])\n",
      "block_list.0.batch_norm_depth.bias torch.Size([32])\n",
      "block_list.0.batch_norm_depth.running_mean torch.Size([32])\n",
      "block_list.0.batch_norm_depth.running_var torch.Size([32])\n",
      "block_list.0.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.0.conv_se_reduce.weight torch.Size([8, 32, 1, 1])\n",
      "block_list.0.conv_se_reduce.bias torch.Size([8])\n",
      "block_list.0.conv_se_expand.weight torch.Size([32, 8, 1, 1])\n",
      "block_list.0.conv_se_expand.bias torch.Size([32])\n",
      "block_list.0.conv_output.weight torch.Size([16, 32, 1, 1])\n",
      "block_list.0.batch_norm_last.weight torch.Size([16])\n",
      "block_list.0.batch_norm_last.bias torch.Size([16])\n",
      "block_list.0.batch_norm_last.running_mean torch.Size([16])\n",
      "block_list.0.batch_norm_last.running_var torch.Size([16])\n",
      "block_list.0.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.1.conv_expand.weight torch.Size([96, 16, 1, 1])\n",
      "block_list.1.batch_norm_exp.weight torch.Size([96])\n",
      "block_list.1.batch_norm_exp.bias torch.Size([96])\n",
      "block_list.1.batch_norm_exp.running_mean torch.Size([96])\n",
      "block_list.1.batch_norm_exp.running_var torch.Size([96])\n",
      "block_list.1.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.1.depth_conv.weight torch.Size([96, 1, 3, 3])\n",
      "block_list.1.batch_norm_depth.weight torch.Size([96])\n",
      "block_list.1.batch_norm_depth.bias torch.Size([96])\n",
      "block_list.1.batch_norm_depth.running_mean torch.Size([96])\n",
      "block_list.1.batch_norm_depth.running_var torch.Size([96])\n",
      "block_list.1.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.1.conv_se_reduce.weight torch.Size([4, 96, 1, 1])\n",
      "block_list.1.conv_se_reduce.bias torch.Size([4])\n",
      "block_list.1.conv_se_expand.weight torch.Size([96, 4, 1, 1])\n",
      "block_list.1.conv_se_expand.bias torch.Size([96])\n",
      "block_list.1.conv_output.weight torch.Size([24, 96, 1, 1])\n",
      "block_list.1.batch_norm_last.weight torch.Size([24])\n",
      "block_list.1.batch_norm_last.bias torch.Size([24])\n",
      "block_list.1.batch_norm_last.running_mean torch.Size([24])\n",
      "block_list.1.batch_norm_last.running_var torch.Size([24])\n",
      "block_list.1.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.2.conv_expand.weight torch.Size([144, 24, 1, 1])\n",
      "block_list.2.batch_norm_exp.weight torch.Size([144])\n",
      "block_list.2.batch_norm_exp.bias torch.Size([144])\n",
      "block_list.2.batch_norm_exp.running_mean torch.Size([144])\n",
      "block_list.2.batch_norm_exp.running_var torch.Size([144])\n",
      "block_list.2.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.2.depth_conv.weight torch.Size([144, 1, 3, 3])\n",
      "block_list.2.batch_norm_depth.weight torch.Size([144])\n",
      "block_list.2.batch_norm_depth.bias torch.Size([144])\n",
      "block_list.2.batch_norm_depth.running_mean torch.Size([144])\n",
      "block_list.2.batch_norm_depth.running_var torch.Size([144])\n",
      "block_list.2.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.2.conv_se_reduce.weight torch.Size([6, 144, 1, 1])\n",
      "block_list.2.conv_se_reduce.bias torch.Size([6])\n",
      "block_list.2.conv_se_expand.weight torch.Size([144, 6, 1, 1])\n",
      "block_list.2.conv_se_expand.bias torch.Size([144])\n",
      "block_list.2.conv_output.weight torch.Size([24, 144, 1, 1])\n",
      "block_list.2.batch_norm_last.weight torch.Size([24])\n",
      "block_list.2.batch_norm_last.bias torch.Size([24])\n",
      "block_list.2.batch_norm_last.running_mean torch.Size([24])\n",
      "block_list.2.batch_norm_last.running_var torch.Size([24])\n",
      "block_list.2.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.3.conv_expand.weight torch.Size([144, 24, 1, 1])\n",
      "block_list.3.batch_norm_exp.weight torch.Size([144])\n",
      "block_list.3.batch_norm_exp.bias torch.Size([144])\n",
      "block_list.3.batch_norm_exp.running_mean torch.Size([144])\n",
      "block_list.3.batch_norm_exp.running_var torch.Size([144])\n",
      "block_list.3.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.3.depth_conv.weight torch.Size([144, 1, 5, 5])\n",
      "block_list.3.batch_norm_depth.weight torch.Size([144])\n",
      "block_list.3.batch_norm_depth.bias torch.Size([144])\n",
      "block_list.3.batch_norm_depth.running_mean torch.Size([144])\n",
      "block_list.3.batch_norm_depth.running_var torch.Size([144])\n",
      "block_list.3.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.3.conv_se_reduce.weight torch.Size([6, 144, 1, 1])\n",
      "block_list.3.conv_se_reduce.bias torch.Size([6])\n",
      "block_list.3.conv_se_expand.weight torch.Size([144, 6, 1, 1])\n",
      "block_list.3.conv_se_expand.bias torch.Size([144])\n",
      "block_list.3.conv_output.weight torch.Size([40, 144, 1, 1])\n",
      "block_list.3.batch_norm_last.weight torch.Size([40])\n",
      "block_list.3.batch_norm_last.bias torch.Size([40])\n",
      "block_list.3.batch_norm_last.running_mean torch.Size([40])\n",
      "block_list.3.batch_norm_last.running_var torch.Size([40])\n",
      "block_list.3.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.4.conv_expand.weight torch.Size([240, 40, 1, 1])\n",
      "block_list.4.batch_norm_exp.weight torch.Size([240])\n",
      "block_list.4.batch_norm_exp.bias torch.Size([240])\n",
      "block_list.4.batch_norm_exp.running_mean torch.Size([240])\n",
      "block_list.4.batch_norm_exp.running_var torch.Size([240])\n",
      "block_list.4.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.4.depth_conv.weight torch.Size([240, 1, 5, 5])\n",
      "block_list.4.batch_norm_depth.weight torch.Size([240])\n",
      "block_list.4.batch_norm_depth.bias torch.Size([240])\n",
      "block_list.4.batch_norm_depth.running_mean torch.Size([240])\n",
      "block_list.4.batch_norm_depth.running_var torch.Size([240])\n",
      "block_list.4.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.4.conv_se_reduce.weight torch.Size([10, 240, 1, 1])\n",
      "block_list.4.conv_se_reduce.bias torch.Size([10])\n",
      "block_list.4.conv_se_expand.weight torch.Size([240, 10, 1, 1])\n",
      "block_list.4.conv_se_expand.bias torch.Size([240])\n",
      "block_list.4.conv_output.weight torch.Size([40, 240, 1, 1])\n",
      "block_list.4.batch_norm_last.weight torch.Size([40])\n",
      "block_list.4.batch_norm_last.bias torch.Size([40])\n",
      "block_list.4.batch_norm_last.running_mean torch.Size([40])\n",
      "block_list.4.batch_norm_last.running_var torch.Size([40])\n",
      "block_list.4.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.5.conv_expand.weight torch.Size([240, 40, 1, 1])\n",
      "block_list.5.batch_norm_exp.weight torch.Size([240])\n",
      "block_list.5.batch_norm_exp.bias torch.Size([240])\n",
      "block_list.5.batch_norm_exp.running_mean torch.Size([240])\n",
      "block_list.5.batch_norm_exp.running_var torch.Size([240])\n",
      "block_list.5.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.5.depth_conv.weight torch.Size([240, 1, 3, 3])\n",
      "block_list.5.batch_norm_depth.weight torch.Size([240])\n",
      "block_list.5.batch_norm_depth.bias torch.Size([240])\n",
      "block_list.5.batch_norm_depth.running_mean torch.Size([240])\n",
      "block_list.5.batch_norm_depth.running_var torch.Size([240])\n",
      "block_list.5.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.5.conv_se_reduce.weight torch.Size([10, 240, 1, 1])\n",
      "block_list.5.conv_se_reduce.bias torch.Size([10])\n",
      "block_list.5.conv_se_expand.weight torch.Size([240, 10, 1, 1])\n",
      "block_list.5.conv_se_expand.bias torch.Size([240])\n",
      "block_list.5.conv_output.weight torch.Size([80, 240, 1, 1])\n",
      "block_list.5.batch_norm_last.weight torch.Size([80])\n",
      "block_list.5.batch_norm_last.bias torch.Size([80])\n",
      "block_list.5.batch_norm_last.running_mean torch.Size([80])\n",
      "block_list.5.batch_norm_last.running_var torch.Size([80])\n",
      "block_list.5.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.6.conv_expand.weight torch.Size([480, 80, 1, 1])\n",
      "block_list.6.batch_norm_exp.weight torch.Size([480])\n",
      "block_list.6.batch_norm_exp.bias torch.Size([480])\n",
      "block_list.6.batch_norm_exp.running_mean torch.Size([480])\n",
      "block_list.6.batch_norm_exp.running_var torch.Size([480])\n",
      "block_list.6.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.6.depth_conv.weight torch.Size([480, 1, 3, 3])\n",
      "block_list.6.batch_norm_depth.weight torch.Size([480])\n",
      "block_list.6.batch_norm_depth.bias torch.Size([480])\n",
      "block_list.6.batch_norm_depth.running_mean torch.Size([480])\n",
      "block_list.6.batch_norm_depth.running_var torch.Size([480])\n",
      "block_list.6.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.6.conv_se_reduce.weight torch.Size([20, 480, 1, 1])\n",
      "block_list.6.conv_se_reduce.bias torch.Size([20])\n",
      "block_list.6.conv_se_expand.weight torch.Size([480, 20, 1, 1])\n",
      "block_list.6.conv_se_expand.bias torch.Size([480])\n",
      "block_list.6.conv_output.weight torch.Size([80, 480, 1, 1])\n",
      "block_list.6.batch_norm_last.weight torch.Size([80])\n",
      "block_list.6.batch_norm_last.bias torch.Size([80])\n",
      "block_list.6.batch_norm_last.running_mean torch.Size([80])\n",
      "block_list.6.batch_norm_last.running_var torch.Size([80])\n",
      "block_list.6.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.7.conv_expand.weight torch.Size([480, 80, 1, 1])\n",
      "block_list.7.batch_norm_exp.weight torch.Size([480])\n",
      "block_list.7.batch_norm_exp.bias torch.Size([480])\n",
      "block_list.7.batch_norm_exp.running_mean torch.Size([480])\n",
      "block_list.7.batch_norm_exp.running_var torch.Size([480])\n",
      "block_list.7.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.7.depth_conv.weight torch.Size([480, 1, 3, 3])\n",
      "block_list.7.batch_norm_depth.weight torch.Size([480])\n",
      "block_list.7.batch_norm_depth.bias torch.Size([480])\n",
      "block_list.7.batch_norm_depth.running_mean torch.Size([480])\n",
      "block_list.7.batch_norm_depth.running_var torch.Size([480])\n",
      "block_list.7.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.7.conv_se_reduce.weight torch.Size([20, 480, 1, 1])\n",
      "block_list.7.conv_se_reduce.bias torch.Size([20])\n",
      "block_list.7.conv_se_expand.weight torch.Size([480, 20, 1, 1])\n",
      "block_list.7.conv_se_expand.bias torch.Size([480])\n",
      "block_list.7.conv_output.weight torch.Size([80, 480, 1, 1])\n",
      "block_list.7.batch_norm_last.weight torch.Size([80])\n",
      "block_list.7.batch_norm_last.bias torch.Size([80])\n",
      "block_list.7.batch_norm_last.running_mean torch.Size([80])\n",
      "block_list.7.batch_norm_last.running_var torch.Size([80])\n",
      "block_list.7.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.8.conv_expand.weight torch.Size([480, 80, 1, 1])\n",
      "block_list.8.batch_norm_exp.weight torch.Size([480])\n",
      "block_list.8.batch_norm_exp.bias torch.Size([480])\n",
      "block_list.8.batch_norm_exp.running_mean torch.Size([480])\n",
      "block_list.8.batch_norm_exp.running_var torch.Size([480])\n",
      "block_list.8.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.8.depth_conv.weight torch.Size([480, 1, 5, 5])\n",
      "block_list.8.batch_norm_depth.weight torch.Size([480])\n",
      "block_list.8.batch_norm_depth.bias torch.Size([480])\n",
      "block_list.8.batch_norm_depth.running_mean torch.Size([480])\n",
      "block_list.8.batch_norm_depth.running_var torch.Size([480])\n",
      "block_list.8.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.8.conv_se_reduce.weight torch.Size([20, 480, 1, 1])\n",
      "block_list.8.conv_se_reduce.bias torch.Size([20])\n",
      "block_list.8.conv_se_expand.weight torch.Size([480, 20, 1, 1])\n",
      "block_list.8.conv_se_expand.bias torch.Size([480])\n",
      "block_list.8.conv_output.weight torch.Size([112, 480, 1, 1])\n",
      "block_list.8.batch_norm_last.weight torch.Size([112])\n",
      "block_list.8.batch_norm_last.bias torch.Size([112])\n",
      "block_list.8.batch_norm_last.running_mean torch.Size([112])\n",
      "block_list.8.batch_norm_last.running_var torch.Size([112])\n",
      "block_list.8.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.9.conv_expand.weight torch.Size([672, 112, 1, 1])\n",
      "block_list.9.batch_norm_exp.weight torch.Size([672])\n",
      "block_list.9.batch_norm_exp.bias torch.Size([672])\n",
      "block_list.9.batch_norm_exp.running_mean torch.Size([672])\n",
      "block_list.9.batch_norm_exp.running_var torch.Size([672])\n",
      "block_list.9.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.9.depth_conv.weight torch.Size([672, 1, 5, 5])\n",
      "block_list.9.batch_norm_depth.weight torch.Size([672])\n",
      "block_list.9.batch_norm_depth.bias torch.Size([672])\n",
      "block_list.9.batch_norm_depth.running_mean torch.Size([672])\n",
      "block_list.9.batch_norm_depth.running_var torch.Size([672])\n",
      "block_list.9.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.9.conv_se_reduce.weight torch.Size([28, 672, 1, 1])\n",
      "block_list.9.conv_se_reduce.bias torch.Size([28])\n",
      "block_list.9.conv_se_expand.weight torch.Size([672, 28, 1, 1])\n",
      "block_list.9.conv_se_expand.bias torch.Size([672])\n",
      "block_list.9.conv_output.weight torch.Size([112, 672, 1, 1])\n",
      "block_list.9.batch_norm_last.weight torch.Size([112])\n",
      "block_list.9.batch_norm_last.bias torch.Size([112])\n",
      "block_list.9.batch_norm_last.running_mean torch.Size([112])\n",
      "block_list.9.batch_norm_last.running_var torch.Size([112])\n",
      "block_list.9.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.10.conv_expand.weight torch.Size([672, 112, 1, 1])\n",
      "block_list.10.batch_norm_exp.weight torch.Size([672])\n",
      "block_list.10.batch_norm_exp.bias torch.Size([672])\n",
      "block_list.10.batch_norm_exp.running_mean torch.Size([672])\n",
      "block_list.10.batch_norm_exp.running_var torch.Size([672])\n",
      "block_list.10.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.10.depth_conv.weight torch.Size([672, 1, 5, 5])\n",
      "block_list.10.batch_norm_depth.weight torch.Size([672])\n",
      "block_list.10.batch_norm_depth.bias torch.Size([672])\n",
      "block_list.10.batch_norm_depth.running_mean torch.Size([672])\n",
      "block_list.10.batch_norm_depth.running_var torch.Size([672])\n",
      "block_list.10.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.10.conv_se_reduce.weight torch.Size([28, 672, 1, 1])\n",
      "block_list.10.conv_se_reduce.bias torch.Size([28])\n",
      "block_list.10.conv_se_expand.weight torch.Size([672, 28, 1, 1])\n",
      "block_list.10.conv_se_expand.bias torch.Size([672])\n",
      "block_list.10.conv_output.weight torch.Size([112, 672, 1, 1])\n",
      "block_list.10.batch_norm_last.weight torch.Size([112])\n",
      "block_list.10.batch_norm_last.bias torch.Size([112])\n",
      "block_list.10.batch_norm_last.running_mean torch.Size([112])\n",
      "block_list.10.batch_norm_last.running_var torch.Size([112])\n",
      "block_list.10.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.11.conv_expand.weight torch.Size([672, 112, 1, 1])\n",
      "block_list.11.batch_norm_exp.weight torch.Size([672])\n",
      "block_list.11.batch_norm_exp.bias torch.Size([672])\n",
      "block_list.11.batch_norm_exp.running_mean torch.Size([672])\n",
      "block_list.11.batch_norm_exp.running_var torch.Size([672])\n",
      "block_list.11.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.11.depth_conv.weight torch.Size([672, 1, 5, 5])\n",
      "block_list.11.batch_norm_depth.weight torch.Size([672])\n",
      "block_list.11.batch_norm_depth.bias torch.Size([672])\n",
      "block_list.11.batch_norm_depth.running_mean torch.Size([672])\n",
      "block_list.11.batch_norm_depth.running_var torch.Size([672])\n",
      "block_list.11.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.11.conv_se_reduce.weight torch.Size([28, 672, 1, 1])\n",
      "block_list.11.conv_se_reduce.bias torch.Size([28])\n",
      "block_list.11.conv_se_expand.weight torch.Size([672, 28, 1, 1])\n",
      "block_list.11.conv_se_expand.bias torch.Size([672])\n",
      "block_list.11.conv_output.weight torch.Size([192, 672, 1, 1])\n",
      "block_list.11.batch_norm_last.weight torch.Size([192])\n",
      "block_list.11.batch_norm_last.bias torch.Size([192])\n",
      "block_list.11.batch_norm_last.running_mean torch.Size([192])\n",
      "block_list.11.batch_norm_last.running_var torch.Size([192])\n",
      "block_list.11.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.12.conv_expand.weight torch.Size([1152, 192, 1, 1])\n",
      "block_list.12.batch_norm_exp.weight torch.Size([1152])\n",
      "block_list.12.batch_norm_exp.bias torch.Size([1152])\n",
      "block_list.12.batch_norm_exp.running_mean torch.Size([1152])\n",
      "block_list.12.batch_norm_exp.running_var torch.Size([1152])\n",
      "block_list.12.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.12.depth_conv.weight torch.Size([1152, 1, 5, 5])\n",
      "block_list.12.batch_norm_depth.weight torch.Size([1152])\n",
      "block_list.12.batch_norm_depth.bias torch.Size([1152])\n",
      "block_list.12.batch_norm_depth.running_mean torch.Size([1152])\n",
      "block_list.12.batch_norm_depth.running_var torch.Size([1152])\n",
      "block_list.12.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.12.conv_se_reduce.weight torch.Size([48, 1152, 1, 1])\n",
      "block_list.12.conv_se_reduce.bias torch.Size([48])\n",
      "block_list.12.conv_se_expand.weight torch.Size([1152, 48, 1, 1])\n",
      "block_list.12.conv_se_expand.bias torch.Size([1152])\n",
      "block_list.12.conv_output.weight torch.Size([192, 1152, 1, 1])\n",
      "block_list.12.batch_norm_last.weight torch.Size([192])\n",
      "block_list.12.batch_norm_last.bias torch.Size([192])\n",
      "block_list.12.batch_norm_last.running_mean torch.Size([192])\n",
      "block_list.12.batch_norm_last.running_var torch.Size([192])\n",
      "block_list.12.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.13.conv_expand.weight torch.Size([1152, 192, 1, 1])\n",
      "block_list.13.batch_norm_exp.weight torch.Size([1152])\n",
      "block_list.13.batch_norm_exp.bias torch.Size([1152])\n",
      "block_list.13.batch_norm_exp.running_mean torch.Size([1152])\n",
      "block_list.13.batch_norm_exp.running_var torch.Size([1152])\n",
      "block_list.13.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.13.depth_conv.weight torch.Size([1152, 1, 5, 5])\n",
      "block_list.13.batch_norm_depth.weight torch.Size([1152])\n",
      "block_list.13.batch_norm_depth.bias torch.Size([1152])\n",
      "block_list.13.batch_norm_depth.running_mean torch.Size([1152])\n",
      "block_list.13.batch_norm_depth.running_var torch.Size([1152])\n",
      "block_list.13.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.13.conv_se_reduce.weight torch.Size([48, 1152, 1, 1])\n",
      "block_list.13.conv_se_reduce.bias torch.Size([48])\n",
      "block_list.13.conv_se_expand.weight torch.Size([1152, 48, 1, 1])\n",
      "block_list.13.conv_se_expand.bias torch.Size([1152])\n",
      "block_list.13.conv_output.weight torch.Size([192, 1152, 1, 1])\n",
      "block_list.13.batch_norm_last.weight torch.Size([192])\n",
      "block_list.13.batch_norm_last.bias torch.Size([192])\n",
      "block_list.13.batch_norm_last.running_mean torch.Size([192])\n",
      "block_list.13.batch_norm_last.running_var torch.Size([192])\n",
      "block_list.13.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.14.conv_expand.weight torch.Size([1152, 192, 1, 1])\n",
      "block_list.14.batch_norm_exp.weight torch.Size([1152])\n",
      "block_list.14.batch_norm_exp.bias torch.Size([1152])\n",
      "block_list.14.batch_norm_exp.running_mean torch.Size([1152])\n",
      "block_list.14.batch_norm_exp.running_var torch.Size([1152])\n",
      "block_list.14.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.14.depth_conv.weight torch.Size([1152, 1, 5, 5])\n",
      "block_list.14.batch_norm_depth.weight torch.Size([1152])\n",
      "block_list.14.batch_norm_depth.bias torch.Size([1152])\n",
      "block_list.14.batch_norm_depth.running_mean torch.Size([1152])\n",
      "block_list.14.batch_norm_depth.running_var torch.Size([1152])\n",
      "block_list.14.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.14.conv_se_reduce.weight torch.Size([48, 1152, 1, 1])\n",
      "block_list.14.conv_se_reduce.bias torch.Size([48])\n",
      "block_list.14.conv_se_expand.weight torch.Size([1152, 48, 1, 1])\n",
      "block_list.14.conv_se_expand.bias torch.Size([1152])\n",
      "block_list.14.conv_output.weight torch.Size([192, 1152, 1, 1])\n",
      "block_list.14.batch_norm_last.weight torch.Size([192])\n",
      "block_list.14.batch_norm_last.bias torch.Size([192])\n",
      "block_list.14.batch_norm_last.running_mean torch.Size([192])\n",
      "block_list.14.batch_norm_last.running_var torch.Size([192])\n",
      "block_list.14.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "block_list.15.conv_expand.weight torch.Size([1152, 192, 1, 1])\n",
      "block_list.15.batch_norm_exp.weight torch.Size([1152])\n",
      "block_list.15.batch_norm_exp.bias torch.Size([1152])\n",
      "block_list.15.batch_norm_exp.running_mean torch.Size([1152])\n",
      "block_list.15.batch_norm_exp.running_var torch.Size([1152])\n",
      "block_list.15.batch_norm_exp.num_batches_tracked torch.Size([])\n",
      "block_list.15.depth_conv.weight torch.Size([1152, 1, 3, 3])\n",
      "block_list.15.batch_norm_depth.weight torch.Size([1152])\n",
      "block_list.15.batch_norm_depth.bias torch.Size([1152])\n",
      "block_list.15.batch_norm_depth.running_mean torch.Size([1152])\n",
      "block_list.15.batch_norm_depth.running_var torch.Size([1152])\n",
      "block_list.15.batch_norm_depth.num_batches_tracked torch.Size([])\n",
      "block_list.15.conv_se_reduce.weight torch.Size([48, 1152, 1, 1])\n",
      "block_list.15.conv_se_reduce.bias torch.Size([48])\n",
      "block_list.15.conv_se_expand.weight torch.Size([1152, 48, 1, 1])\n",
      "block_list.15.conv_se_expand.bias torch.Size([1152])\n",
      "block_list.15.conv_output.weight torch.Size([320, 1152, 1, 1])\n",
      "block_list.15.batch_norm_last.weight torch.Size([320])\n",
      "block_list.15.batch_norm_last.bias torch.Size([320])\n",
      "block_list.15.batch_norm_last.running_mean torch.Size([320])\n",
      "block_list.15.batch_norm_last.running_var torch.Size([320])\n",
      "block_list.15.batch_norm_last.num_batches_tracked torch.Size([])\n",
      "conv_final.weight torch.Size([1280, 320, 1, 1])\n",
      "batch_norm_final.weight torch.Size([1280])\n",
      "batch_norm_final.bias torch.Size([1280])\n",
      "batch_norm_final.running_mean torch.Size([1280])\n",
      "batch_norm_final.running_var torch.Size([1280])\n",
      "batch_norm_final.num_batches_tracked torch.Size([])\n",
      "linear.weight torch.Size([1000, 1280])\n",
      "linear.bias torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "for k,v in effnet.state_dict().items():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ea30785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the state dictionary from the pytorch model to be editted\n",
    "sd = effnet.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8c66005",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_pytorch_convert = {'se_reduce':'conv_se_reduce','se_expand':'conv_se_expand','project_conv':'conv_output',\n",
    "                       'expand_conv':'conv_expand', 'dwconv':'depth_conv','top_conv':'conv_final','probs':'linear',\n",
    "                      'stem_conv':'conv1', 'stem_bn':'batch_norm1', 'top_bn':'batch_norm_final','expand_bn':'batch_norm_exp',\n",
    "                        'bn':'batch_norm_depth','project_bn':'batch_norm_last'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c1e10f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesnt work on depth conv\n",
    "# transpose works on everything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eeb2e04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "stem_conv\n",
      "==========================================\n",
      "stem_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block1a_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block1a_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block1a_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block1a_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block1a_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block1a_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2a_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2a_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2a_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2a_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2a_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2a_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2a_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2a_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2b_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2b_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2b_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2b_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2b_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2b_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2b_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block2b_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3a_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3a_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3a_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3a_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3a_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3a_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3a_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3a_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3b_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3b_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3b_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3b_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3b_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3b_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3b_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block3b_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4a_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4a_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4a_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4a_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4a_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4a_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4a_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4a_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4b_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4b_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4b_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4b_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4b_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4b_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4b_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4b_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4c_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4c_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4c_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4c_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4c_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4c_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4c_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block4c_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5a_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5a_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5a_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5a_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5a_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5a_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5a_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5a_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5b_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5b_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5b_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5b_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5b_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5b_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5b_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5b_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5c_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5c_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5c_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5c_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5c_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5c_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5c_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block5c_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6a_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6a_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6a_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6a_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6a_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6a_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6a_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6a_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6b_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6b_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6b_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6b_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6b_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6b_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6b_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6b_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6c_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6c_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6c_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6c_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6c_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6c_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6c_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6c_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6d_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6d_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6d_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6d_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6d_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6d_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6d_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block6d_project_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block7a_expand_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block7a_expand_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block7a_dwconv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block7a_bn\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block7a_se_reduce\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block7a_se_expand\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block7a_project_conv\n",
      "==========================================\n",
      "keras\n",
      "pytorch\n",
      "block7a_project_bn\n",
      "==========================================\n",
      "top_conv\n",
      "==========================================\n",
      "top_bn\n",
      "==========================================\n",
      "probs\n"
     ]
    }
   ],
   "source": [
    "layer = -1\n",
    "name = ''\n",
    "for i,l in enumerate(model.layers):\n",
    "    if l.get_weights():\n",
    "        org_name = l.get_config()['name']\n",
    "        weights = l.get_weights()\n",
    "        # any weights in a block unit\n",
    "        if org_name[0:5] == 'block':\n",
    "            if org_name[5:7] != name:\n",
    "                layer += 1\n",
    "                name=org_name[5:7]\n",
    "            base_pyt = 'block_list.'\n",
    "            print('======'*7)\n",
    "            print('keras')\n",
    "            print('pytorch')\n",
    "            print(org_name)\n",
    "            if keras_pytorch_convert.get(org_name[8:]):\n",
    "                use = keras_pytorch_convert[org_name[8:]]\n",
    "                py_name = f'{base_pyt}{layer}.{use}'\n",
    "                if org_name[8:] == 'dwconv':\n",
    "                    new_weights = torch.from_numpy(weights[0]).permute(2,3,0,1)\n",
    "                    sd[py_name+'.weight'] = new_weights\n",
    "                # any non batchnorm or non depthconv layer\n",
    "                elif 'bn' not in org_name:\n",
    "                    new_weights = weights[0]\n",
    "                    if 'conv' in org_name:\n",
    "                        new_weights = torch.from_numpy(np.moveaxis(new_weights, [-1, -2], [0, 1]))\n",
    "                    else:\n",
    "                        new_weights = torch.from_numpy(new_weights.T)\n",
    "                    sd[py_name+'.weight'] = new_weights\n",
    "                    # this means theres a bias to account for\n",
    "                    if len(weights)>1:\n",
    "                        new_weights = weights[1]\n",
    "                        new_weights = torch.from_numpy(new_weights)\n",
    "                        sd[py_name+'.bias'] = new_weights\n",
    "                # its a batchnorm layer\n",
    "                else:\n",
    "                    new_weights = weights[0]\n",
    "                    new_weights = torch.from_numpy(new_weights)\n",
    "                    sd[py_name+'.weight'] = new_weights\n",
    "                    # this means theres a bias to account for\n",
    "                    if len(weights)>1:\n",
    "                        new_weights = [torch.from_numpy(i) for i in weights]\n",
    "                        sd[py_name+'.bias'] = new_weights[1]\n",
    "                        sd[py_name+'.running_mean'] = new_weights[2]\n",
    "                        sd[py_name+'.running_var'] = new_weights[3]\n",
    "            else:\n",
    "                continue\n",
    "        # initial convs or final convs\n",
    "        else:\n",
    "            print('======'*7)\n",
    "            print(org_name)\n",
    "            py_name = keras_pytorch_convert[org_name]\n",
    "            if 'bn' in org_name:\n",
    "                new_weights = torch.from_numpy(weights[0])\n",
    "            else:\n",
    "                new_weights = torch.from_numpy(np.moveaxis(weights[0], [-1, -2], [0, 1]))\n",
    "            sd[py_name+'.weight'] = new_weights\n",
    "            # handle bias weights as well\n",
    "            if len(weights) > 1:\n",
    "                new_weights = [torch.from_numpy(i) for i in weights]\n",
    "                sd[py_name+'.bias'] = new_weights[1]\n",
    "                if org_name == 'probs':\n",
    "                    continue\n",
    "                sd[py_name+'.running_mean'] = new_weights[2]\n",
    "                sd[py_name+'.running_var'] = new_weights[3]\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9bd5a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effnet.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0c588c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3) torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(224,224,3)\n",
    "# x = np.ones((224,224,3))\n",
    "x = np.array([x])\n",
    "x_t = torch.from_numpy(x)\n",
    "x_t = x_t.permute(0,3,1,2).float()\n",
    "print(x.shape, x_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24a40b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = keras.Model(inputs=model.inputs,\n",
    "                        outputs={layer.get_config()['name']:layer.output for layer in model.layers})\n",
    "features = extractor.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49efa147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-2.7138e-02, -4.0898e-02, -2.3357e-02,  ..., -3.9108e-02,\n",
      "           -3.7071e-02, -4.2329e-02],\n",
      "          [-4.3952e-02, -5.3096e-02, -2.7443e-02,  ..., -4.7519e-02,\n",
      "           -5.3321e-02, -2.4630e-02],\n",
      "          [-4.2403e-02, -3.2150e-02, -3.4504e-02,  ..., -2.9217e-02,\n",
      "           -3.4150e-02, -3.4687e-02],\n",
      "          ...,\n",
      "          [-3.5080e-02, -3.2896e-02, -2.6630e-02,  ..., -4.0227e-02,\n",
      "           -3.1879e-02, -3.6721e-02],\n",
      "          [-3.5360e-02, -5.1341e-02, -4.2400e-02,  ..., -4.1989e-02,\n",
      "           -4.0895e-02, -5.1079e-02],\n",
      "          [-2.7007e-02, -5.5312e-02, -5.0080e-02,  ..., -3.8300e-02,\n",
      "           -3.8028e-02, -4.8179e-02]],\n",
      "\n",
      "         [[-1.4770e-02,  1.4252e+00,  3.6046e+00,  ...,  6.2116e+00,\n",
      "            4.0492e+00, -2.4787e-01],\n",
      "          [-2.6463e-01,  2.7693e+00,  3.7082e+00,  ...,  1.6728e+00,\n",
      "            2.9753e+00,  7.5984e-01],\n",
      "          [ 2.0398e+00,  5.7819e+00,  2.9327e+00,  ...,  3.5580e+00,\n",
      "            2.4147e+00, -1.0050e-02],\n",
      "          ...,\n",
      "          [ 2.2521e+00,  2.0188e+00, -1.0575e-01,  ...,  2.0166e+00,\n",
      "            3.7951e+00,  2.5335e+00],\n",
      "          [ 3.1189e+00,  1.2024e+00,  1.9621e+00,  ...,  6.9039e-01,\n",
      "            2.1078e+00,  3.9610e+00],\n",
      "          [ 3.7095e+00,  2.4086e+00,  4.7144e+00,  ...,  3.8991e+00,\n",
      "            3.9564e+00,  4.0065e+00]],\n",
      "\n",
      "         [[ 5.7977e+00,  3.2040e+00,  8.9362e-01,  ..., -2.7669e-01,\n",
      "            2.4542e-01,  6.2911e+00],\n",
      "          [ 5.3221e+00,  1.8394e+00,  5.2130e-01,  ...,  2.4053e+00,\n",
      "            1.1686e+00,  3.5421e+00],\n",
      "          [ 2.2376e+00, -1.9466e-01,  1.7322e+00,  ...,  4.0712e-01,\n",
      "            2.3522e+00,  4.5862e+00],\n",
      "          ...,\n",
      "          [ 3.0489e+00,  1.9805e+00,  5.1936e+00,  ...,  2.4616e+00,\n",
      "            4.1050e-01,  1.9137e+00],\n",
      "          [ 1.7169e+00,  2.4315e+00,  2.5386e+00,  ...,  3.9313e+00,\n",
      "            2.9855e+00,  9.4425e-01],\n",
      "          [ 1.0783e+00,  1.0036e+00, -1.6570e-01,  ...,  5.0210e-01,\n",
      "            4.6038e-01,  3.4953e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.4397e+00,  2.2544e+00,  2.4046e+00,  ...,  2.0501e+00,\n",
      "            2.4360e+00,  2.9110e+00],\n",
      "          [ 2.8132e+00,  1.7122e+00,  2.1873e+00,  ...,  2.5925e+00,\n",
      "            2.1419e+00,  2.6090e+00],\n",
      "          [ 2.1377e+00,  1.6179e+00,  2.3122e+00,  ...,  2.6125e+00,\n",
      "            2.5925e+00,  2.9107e+00],\n",
      "          ...,\n",
      "          [ 2.0018e+00,  2.3475e+00,  2.5435e+00,  ...,  2.2891e+00,\n",
      "            2.2028e+00,  1.6689e+00],\n",
      "          [ 2.2767e+00,  2.5681e+00,  2.0678e+00,  ...,  2.3366e+00,\n",
      "            2.3975e+00,  2.0913e+00],\n",
      "          [ 2.2641e+00,  2.5378e+00,  1.9615e+00,  ...,  2.4355e+00,\n",
      "            2.3841e+00,  2.2236e+00]],\n",
      "\n",
      "         [[-4.3592e-06, -2.2145e-05, -8.8712e-06,  ..., -2.2282e-05,\n",
      "           -2.2033e-05, -2.0207e-05],\n",
      "          [-1.5734e-05, -5.7729e-05, -8.0217e-06,  ..., -3.1066e-05,\n",
      "           -8.1307e-05, -8.2281e-06],\n",
      "          [-3.3730e-05, -2.9681e-05, -1.5867e-05,  ..., -1.8360e-05,\n",
      "           -7.2583e-06, -5.9144e-06],\n",
      "          ...,\n",
      "          [-1.4818e-05, -1.4144e-05, -1.1798e-05,  ..., -1.2597e-05,\n",
      "           -1.3257e-05, -2.3175e-05],\n",
      "          [-2.3944e-05, -4.9846e-05, -5.2021e-05,  ..., -2.0583e-05,\n",
      "           -4.2893e-05, -5.6343e-05],\n",
      "          [-1.1837e-05, -4.9039e-05, -4.1215e-05,  ..., -1.7246e-05,\n",
      "           -1.9803e-05, -9.1659e-05]],\n",
      "\n",
      "         [[-6.0654e-02,  3.5483e+00,  6.2461e+00,  ..., -1.2768e-01,\n",
      "            4.4660e+00,  2.3868e+00],\n",
      "          [ 2.8424e-01,  6.4108e-01,  9.5209e-01,  ...,  3.4567e+00,\n",
      "            3.0964e-01,  5.9136e+00],\n",
      "          [ 1.0140e+00,  3.6768e+00,  5.1439e+00,  ...,  2.9707e+00,\n",
      "            1.6923e+00,  2.9392e+00],\n",
      "          ...,\n",
      "          [-2.7844e-01,  4.3453e+00,  4.5758e+00,  ...,  1.3552e+00,\n",
      "            3.9986e+00,  2.7182e+00],\n",
      "          [ 1.3244e+00,  1.6522e+00,  2.7018e+00,  ...,  2.4884e+00,\n",
      "            2.6146e+00,  3.9621e+00],\n",
      "          [ 3.1793e+00, -1.5059e-02, -2.7763e-01,  ...,  1.5655e+00,\n",
      "            1.4008e-01,  2.1450e+00]]]], grad_fn=<SiluBackward0>)\n",
      "<built-in method type of Tensor object at 0x000001784D0E0DB0>\n",
      "tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  2.7944e+00,  3.7071e+00,  ...,  2.9193e+00,\n",
      "            5.1547e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  3.8203e+00,  3.1737e+00,  ...,  3.0571e+00,\n",
      "            4.8168e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  2.2523e+00,  3.8568e+00,  ...,  2.9045e+00,\n",
      "            3.6974e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  2.9351e+00,  4.0960e+00,  ...,  2.8367e+00,\n",
      "            4.0516e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  2.4867e+00,  3.6269e+00,  ...,  2.2442e+00,\n",
      "            3.5689e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  2.3822e+00,  2.6000e+00,  ...,  1.9730e+00,\n",
      "            3.3133e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  1.6155e+00,  2.7397e+00,  ...,  2.7882e+00,\n",
      "            4.6068e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  4.1303e+00,  1.5145e+00,  ...,  2.7609e+00,\n",
      "            3.5331e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00, -1.4204e-01, -1.1913e-01,  ..., -1.1419e-01,\n",
      "            8.6282e-02,  0.0000e+00],\n",
      "          [ 0.0000e+00, -9.0979e-02, -1.5137e-01,  ..., -1.2005e-01,\n",
      "           -1.6042e-02,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00, -1.4791e-01, -5.4451e-02,  ..., -1.3592e-01,\n",
      "            2.8768e-02,  0.0000e+00],\n",
      "          [ 0.0000e+00, -7.3599e-02, -2.1479e-02,  ..., -2.4301e-03,\n",
      "            1.9065e-01,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  3.1032e+00,  3.5670e+00,  ..., -1.5225e-01,\n",
      "            2.0654e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00, -2.3738e-01, -2.6645e-01,  ..., -6.9090e-02,\n",
      "            1.4758e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  3.4274e-03, -3.7660e-02,  ...,  1.1679e+00,\n",
      "            2.6556e-01,  0.0000e+00],\n",
      "          [ 0.0000e+00,  9.9698e-01, -1.5693e-03,  ..., -2.1154e-02,\n",
      "           -8.4026e-02,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  1.7559e+00,  1.7489e+00,  ...,  1.1032e+00,\n",
      "            6.4478e-01,  0.0000e+00],\n",
      "          [ 0.0000e+00,  1.5490e+00,  1.7942e+00,  ...,  1.4808e+00,\n",
      "            8.0621e-01,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  1.4707e+00,  9.0878e-01,  ...,  8.0362e-01,\n",
      "            1.4433e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  1.0792e+00,  4.8782e-01,  ...,  6.8966e-01,\n",
      "            1.7405e-01,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  5.3819e+00,  6.1478e+00,  ...,  4.9293e+00,\n",
      "            7.0026e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  6.0518e+00,  6.0829e+00,  ...,  5.7946e+00,\n",
      "            7.6822e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  4.1516e+00,  5.8524e+00,  ...,  5.3652e+00,\n",
      "            7.1450e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  7.5459e+00,  7.2500e+00,  ...,  7.2775e+00,\n",
      "            8.6763e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]], grad_fn=<ConstantPadNdBackward0>)\n",
      "<built-in method type of Tensor object at 0x000001784D7EEE50>\n",
      "tensor([[[[ 0.1361,  0.1362, -0.1353,  ...,  0.3964,  0.9133,  0.3489],\n",
      "          [ 1.2165,  2.4534,  1.8141,  ...,  2.6556,  1.7607,  2.5089],\n",
      "          [ 1.4423,  2.3518,  2.4610,  ...,  3.0974,  2.7883,  2.4175],\n",
      "          ...,\n",
      "          [ 1.8685,  2.4370,  2.1095,  ...,  2.3491,  1.9030,  3.3445],\n",
      "          [ 1.3269,  2.3930,  2.7203,  ...,  1.6675,  2.4270,  2.6713],\n",
      "          [ 1.8386,  2.9257,  3.1180,  ...,  2.3686,  2.9247,  2.4207]],\n",
      "\n",
      "         [[-0.0508, -0.2557, -0.1227,  ..., -0.1412,  0.0299, -0.2754],\n",
      "          [-0.0795, -0.2749, -0.2666,  ..., -0.1053,  0.0725, -0.0242],\n",
      "          [-0.0167,  0.2291, -0.0625,  ..., -0.2275, -0.2039, -0.1221],\n",
      "          ...,\n",
      "          [-0.1306, -0.1312, -0.2173,  ..., -0.2738,  0.0579, -0.0687],\n",
      "          [-0.0129, -0.0550, -0.1247,  ..., -0.2589,  0.6024, -0.2569],\n",
      "          [-0.0039, -0.0218, -0.2083,  ..., -0.0293, -0.0838, -0.2764]],\n",
      "\n",
      "         [[-0.0301,  0.3012,  0.2911,  ...,  0.5003,  0.2101,  0.5493],\n",
      "          [ 0.7094,  2.1421,  2.3151,  ...,  2.0790,  2.3017,  2.3220],\n",
      "          [ 0.7657,  2.3124,  2.3046,  ...,  2.1512,  2.2661,  2.5024],\n",
      "          ...,\n",
      "          [ 0.5895,  2.0970,  2.2769,  ...,  2.0847,  2.1318,  2.1863],\n",
      "          [ 0.4842,  2.2011,  2.1418,  ...,  2.2568,  2.3119,  2.4093],\n",
      "          [ 0.6326,  2.4132,  2.2465,  ...,  2.4674,  2.4819,  2.2116]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0942, -0.1093, -0.2317,  ..., -0.0376, -0.0545, -0.1470],\n",
      "          [-0.2481, -0.2621,  0.8807,  ...,  0.0118,  2.4417,  0.0152],\n",
      "          [-0.2587,  0.2220,  0.8613,  ..., -0.2513, -0.1247,  1.0749],\n",
      "          ...,\n",
      "          [-0.1484, -0.2479,  0.1658,  ...,  0.4624,  0.8289,  0.1384],\n",
      "          [-0.2570,  0.0479, -0.1573,  ...,  0.6064,  1.3326,  1.4404],\n",
      "          [-0.1806,  0.1439, -0.2739,  ...,  0.6802,  0.5107,  1.3273]],\n",
      "\n",
      "         [[ 3.7951,  3.5353,  3.7816,  ...,  3.4254,  3.9079,  3.6730],\n",
      "          [ 3.4412,  3.4689,  3.3127,  ...,  3.3921,  3.2720,  3.4903],\n",
      "          [ 3.4545,  3.6944,  3.3238,  ...,  3.5088,  3.2362,  3.4725],\n",
      "          ...,\n",
      "          [ 3.6382,  3.6354,  3.6624,  ...,  3.3616,  3.2956,  3.0838],\n",
      "          [ 3.6248,  3.2634,  3.5879,  ...,  3.4215,  3.2532,  3.2155],\n",
      "          [ 3.5519,  3.2476,  3.3994,  ...,  3.1239,  3.2975,  3.2051]],\n",
      "\n",
      "         [[-0.1781, -0.2101, -0.2348,  ..., -0.1521, -0.1732, -0.1901],\n",
      "          [-0.1329, -0.0676, -0.2061,  ..., -0.1030, -0.1623, -0.1267],\n",
      "          [-0.1469, -0.1750, -0.1454,  ..., -0.0880, -0.1010, -0.1096],\n",
      "          ...,\n",
      "          [-0.1451, -0.1058, -0.1741,  ..., -0.1386, -0.1591, -0.0323],\n",
      "          [-0.1724, -0.1161, -0.0828,  ..., -0.1830, -0.1410, -0.1189],\n",
      "          [-0.1252, -0.1254, -0.0529,  ..., -0.0970, -0.1003, -0.1347]]]],\n",
      "       grad_fn=<SiluBackward0>)\n",
      "<built-in method type of Tensor object at 0x000001784DAB8220>\n",
      "tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00, -1.0528e-01, -7.6112e-02,  ..., -4.3771e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00, -5.0790e-02, -1.9368e-02,  ...,  3.8408e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00, -3.9882e-02,  8.5326e-03,  ...,  5.1914e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00, -2.4900e-01, -2.4533e-01,  ..., -2.7541e-01,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00, -2.7843e-01, -1.9640e-01,  ..., -2.6712e-01,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00, -2.7776e-01, -1.8600e-01,  ..., -1.8438e-01,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00, -3.3835e-02,  3.0457e-03,  ..., -5.3574e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  6.8669e-03,  4.0857e-02,  ...,  3.4255e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00, -3.1729e-02,  1.1637e-02,  ..., -9.0922e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  3.0612e+00,  2.7854e-01,  ...,  6.8809e-01,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  3.5318e+00,  8.4621e-01,  ...,  1.3484e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  3.9273e+00,  1.5205e+00,  ...,  4.0841e-01,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00, -2.5418e-01, -2.6715e-01,  ..., -2.4720e-01,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00, -2.3240e-01, -1.8830e-01,  ..., -1.3300e-01,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00, -1.0593e-01, -9.7935e-02,  ..., -4.5382e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  1.1400e-01,  3.8414e-02,  ...,  1.7407e-01,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00, -9.5232e-02, -8.9745e-02,  ..., -9.0553e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00, -4.5673e-02, -1.0094e-01,  ..., -4.0407e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]], grad_fn=<ConstantPadNdBackward0>)\n",
      "<built-in method type of Tensor object at 0x000001784DAB8770>\n",
      "tensor([[[[-1.1656e-01, -4.0779e-02,  3.8261e-03,  ...,  6.9563e-03,\n",
      "           -1.4822e-02, -1.0119e-02],\n",
      "          [-1.2588e-01, -1.3541e-02, -5.1648e-02,  ...,  2.5870e-02,\n",
      "           -1.8553e-02,  1.8526e-03],\n",
      "          [-1.4138e-01, -1.4651e-02,  6.3110e-03,  ...,  1.2954e-02,\n",
      "           -1.5424e-02, -2.0764e-02],\n",
      "          ...,\n",
      "          [-1.0045e-01,  4.3687e-02,  3.5542e-04,  ..., -2.8319e-02,\n",
      "           -2.0346e-02, -6.1712e-02],\n",
      "          [-8.8006e-02, -3.4559e-02, -6.0997e-02,  ..., -8.3849e-03,\n",
      "            2.7103e-02,  3.7023e-02],\n",
      "          [-1.1071e-01,  1.7667e-02, -2.1392e-03,  ..., -4.7052e-03,\n",
      "            4.5714e-02,  5.8802e-02]],\n",
      "\n",
      "         [[ 1.8440e-01,  2.6305e-01,  3.1877e-01,  ...,  2.2673e-01,\n",
      "            3.2749e-01,  1.6379e-01],\n",
      "          [-5.6700e-02, -3.8760e-02, -3.4894e-02,  ..., -5.6700e-02,\n",
      "           -1.7120e-02, -7.0821e-02],\n",
      "          [-1.2207e-02, -1.2749e-02, -2.4872e-02,  ..., -3.2365e-02,\n",
      "            8.9315e-04, -8.5410e-03],\n",
      "          ...,\n",
      "          [-4.0617e-02, -6.1869e-02, -5.6733e-02,  ...,  3.8788e-02,\n",
      "           -5.6202e-02, -5.4942e-02],\n",
      "          [-7.1136e-02, -1.5687e-03,  2.9669e-03,  ...,  4.9702e-02,\n",
      "           -1.5954e-02, -2.4703e-02],\n",
      "          [-1.8801e-02, -3.0840e-02,  6.0806e-02,  ...,  1.5316e-04,\n",
      "            1.0883e-02, -3.1509e-02]],\n",
      "\n",
      "         [[ 1.4358e+00,  2.0299e+00,  2.4323e+00,  ...,  1.2028e+00,\n",
      "            2.4420e+00,  1.7781e+00],\n",
      "          [ 6.8214e-01,  9.1617e-01,  3.9391e-01,  ...,  5.2180e-01,\n",
      "            1.4453e+00,  5.0099e-01],\n",
      "          [ 1.4893e+00,  1.2582e+00,  8.5314e-01,  ...,  7.2946e-01,\n",
      "            1.1565e+00,  8.6239e-01],\n",
      "          ...,\n",
      "          [ 1.2207e+00,  7.7545e-01,  4.6074e-01,  ...,  1.7233e+00,\n",
      "            8.6069e-01,  4.6817e-01],\n",
      "          [ 5.8933e-01,  1.2355e+00,  4.8506e-01,  ...,  1.1105e+00,\n",
      "            9.4089e-01,  6.7661e-01],\n",
      "          [ 7.7543e-01,  4.8164e-01,  1.6702e+00,  ...,  7.0802e-01,\n",
      "            9.5794e-01,  6.3033e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.6218e-01, -9.1485e-02, -1.5862e-01,  ...,  1.0775e-01,\n",
      "            9.8373e-02,  1.2093e+00],\n",
      "          [-1.6536e-01,  5.2269e-01,  4.2482e-01,  ...,  5.7580e-01,\n",
      "            1.7474e-01,  1.9332e+00],\n",
      "          [-1.7748e-01,  8.2469e-01,  1.9356e-01,  ...,  5.5356e-01,\n",
      "            2.6833e-01,  1.5896e+00],\n",
      "          ...,\n",
      "          [ 1.5343e-01,  5.6566e-01,  3.9984e-01,  ...,  9.6115e-02,\n",
      "            7.5780e-01,  1.7055e+00],\n",
      "          [ 7.1088e-02,  3.3511e-01,  2.1277e-01,  ..., -1.9057e-01,\n",
      "            1.4145e-01,  1.9596e+00],\n",
      "          [ 1.1390e+00,  1.3966e+00,  1.0243e+00,  ...,  1.0967e+00,\n",
      "            1.1390e+00,  1.5644e+00]],\n",
      "\n",
      "         [[-1.3596e-02, -1.3349e-01, -9.0569e-02,  ..., -8.6923e-02,\n",
      "           -8.3890e-02, -7.7567e-03],\n",
      "          [-3.4517e-02, -1.2719e-01, -1.6281e-01,  ..., -1.6866e-01,\n",
      "           -2.0631e-01, -5.3834e-02],\n",
      "          [-1.2308e-01, -1.9865e-01, -1.8844e-01,  ..., -1.4871e-01,\n",
      "           -1.5440e-01, -3.4142e-02],\n",
      "          ...,\n",
      "          [-6.0672e-02, -1.5905e-01, -1.5814e-01,  ..., -1.7876e-01,\n",
      "           -1.6043e-01, -1.1952e-01],\n",
      "          [-6.7329e-02, -1.6212e-01, -1.7709e-01,  ..., -2.0684e-01,\n",
      "           -1.6911e-01, -5.2637e-02],\n",
      "          [ 3.4706e-03, -1.2583e-01, -1.4728e-01,  ..., -1.6049e-01,\n",
      "           -1.4432e-01, -1.8911e-01]],\n",
      "\n",
      "         [[ 2.3006e-01,  3.6052e-01,  3.5216e-01,  ...,  2.4549e-01,\n",
      "            3.2941e-01,  4.3806e-01],\n",
      "          [ 5.9098e-01,  5.2224e-01,  4.0033e-01,  ...,  5.3923e-01,\n",
      "            6.7703e-01,  6.5293e-01],\n",
      "          [ 5.6719e-01,  6.0075e-01,  5.8837e-01,  ...,  6.2490e-01,\n",
      "            6.1897e-01,  6.3714e-01],\n",
      "          ...,\n",
      "          [ 4.6580e-01,  6.6847e-01,  5.5995e-01,  ...,  7.0498e-01,\n",
      "            6.6860e-01,  6.0000e-01],\n",
      "          [ 4.5419e-01,  5.5426e-01,  5.5320e-01,  ...,  5.5140e-01,\n",
      "            5.1746e-01,  6.2651e-01],\n",
      "          [ 4.5452e-01,  4.1109e-01,  5.6750e-01,  ...,  5.5648e-01,\n",
      "            5.9801e-01,  4.9556e-01]]]], grad_fn=<SiluBackward0>)\n",
      "<built-in method type of Tensor object at 0x000001784DAB8B30>\n",
      "tensor([[[[-0.1601,  0.1417, -0.1128,  ..., -0.0850, -0.1628,  0.0000],\n",
      "          [-0.1000,  0.5654,  0.0379,  ...,  0.5177,  0.0559,  0.0000],\n",
      "          [-0.1161,  0.3762, -0.0624,  ...,  0.2661, -0.0575,  0.0000],\n",
      "          ...,\n",
      "          [-0.1751,  0.3344,  0.1593,  ...,  0.1002, -0.2188,  0.0000],\n",
      "          [-0.1334,  0.1348, -0.1173,  ...,  0.0763,  0.0247,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.1801, -0.1517,  0.0907,  ...,  1.3235, -0.2436,  0.0000],\n",
      "          [-0.1825, -0.0949, -0.2364,  ..., -0.0563, -0.2747,  0.0000],\n",
      "          [-0.2342, -0.2779, -0.1033,  ..., -0.2505, -0.0855,  0.0000],\n",
      "          ...,\n",
      "          [-0.1889, -0.2651, -0.2693,  ..., -0.2767, -0.2335,  0.0000],\n",
      "          [-0.2709, -0.2550, -0.1064,  ..., -0.2544, -0.1069,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.1092,  0.0390, -0.0327,  ...,  0.2329, -0.1981,  0.0000],\n",
      "          [-0.2698,  0.0836, -0.2672,  ..., -0.1989, -0.2292,  0.0000],\n",
      "          [-0.2504, -0.1710, -0.2777,  ..., -0.2344, -0.1558,  0.0000],\n",
      "          ...,\n",
      "          [-0.2708, -0.1299, -0.2525,  ..., -0.1229, -0.2660,  0.0000],\n",
      "          [-0.2583, -0.2049, -0.0338,  ..., -0.2286, -0.2379,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2374, -0.2306, -0.2115,  ..., -0.2441, -0.2779,  0.0000],\n",
      "          [ 0.2518,  0.7713, -0.1033,  ...,  0.1336,  0.2564,  0.0000],\n",
      "          [-0.2604, -0.1865, -0.2681,  ..., -0.1218, -0.2271,  0.0000],\n",
      "          ...,\n",
      "          [-0.0991,  0.2423, -0.2025,  ...,  0.2958,  0.7125,  0.0000],\n",
      "          [ 0.8629,  0.0541, -0.2727,  ...,  0.3103, -0.1108,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.2462,  0.0794, -0.2531,  ..., -0.1319,  1.0074,  0.0000],\n",
      "          [ 0.0203,  2.8690,  1.0304,  ...,  1.9488,  0.9273,  0.0000],\n",
      "          [ 0.0462,  3.7640,  0.4573,  ...,  0.2109,  0.6899,  0.0000],\n",
      "          ...,\n",
      "          [ 0.8121,  1.3185,  0.2290,  ...,  1.3492,  1.1798,  0.0000],\n",
      "          [ 1.5352,  2.0169,  0.6128,  ...,  1.1321,  2.5322,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 3.9378,  2.4262,  2.2201,  ...,  2.9078,  1.9111,  0.0000],\n",
      "          [ 1.6464, -0.1646,  1.3453,  ...,  0.5890, -0.1065,  0.0000],\n",
      "          [ 2.2351,  0.4598,  1.0143,  ...,  0.2354, -0.0057,  0.0000],\n",
      "          ...,\n",
      "          [ 0.9049,  0.7375,  1.7383,  ...,  0.0769,  0.2268,  0.0000],\n",
      "          [ 2.1292,  0.2003,  0.6006,  ...,  0.5143,  1.0058,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],\n",
      "       grad_fn=<ConstantPadNdBackward0>)\n",
      "<built-in method type of Tensor object at 0x000001784DAC5220>\n",
      "tensor([[[[ 2.4882,  1.1198,  0.7681,  ...,  0.9546,  1.0757,  1.2770],\n",
      "          [ 2.9842,  1.7844,  2.0343,  ...,  2.0218,  1.5030,  1.8229],\n",
      "          [ 3.1849,  1.8669,  1.9069,  ...,  1.8300,  1.5474,  2.1193],\n",
      "          ...,\n",
      "          [ 3.0644,  2.0650,  1.6471,  ...,  1.9003,  2.2009,  2.1488],\n",
      "          [ 3.2868,  2.0522,  1.6895,  ...,  1.8765,  2.1682,  1.8187],\n",
      "          [ 2.2362,  0.9952,  0.7358,  ...,  1.4725,  1.5342,  1.1823]],\n",
      "\n",
      "         [[ 3.2191,  0.5963, -0.0923,  ...,  0.0901,  0.1984,  1.1363],\n",
      "          [ 3.9144,  0.5064,  0.8726,  ...,  0.2680,  0.5249,  0.7060],\n",
      "          [ 3.5468,  0.3419,  1.0858,  ...,  0.0461,  0.1709,  1.3403],\n",
      "          ...,\n",
      "          [ 3.4070,  0.1927, -0.0401,  ...,  1.8134,  0.8733,  1.1135],\n",
      "          [ 3.2538,  0.2841, -0.1073,  ...,  1.5969,  0.6243,  1.3429],\n",
      "          [ 1.7907, -0.2357, -0.2361,  ...,  0.1973, -0.0376,  0.4705]],\n",
      "\n",
      "         [[ 1.5450,  1.8829,  2.3055,  ...,  2.2123,  1.7698,  1.7270],\n",
      "          [ 0.7194,  1.0924,  0.7876,  ...,  0.9747,  0.9964,  1.0714],\n",
      "          [ 0.9275,  0.9654,  0.4517,  ...,  0.8916,  1.1556,  0.6950],\n",
      "          ...,\n",
      "          [ 0.7232,  1.1825,  1.3603,  ...,  0.5790,  0.8574,  1.0419],\n",
      "          [ 0.7835,  1.1437,  1.3778,  ...,  0.7865,  0.9113,  0.6613],\n",
      "          [ 1.2369,  1.6757,  1.7342,  ...,  1.1676,  1.3292,  1.4311]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4447,  0.1776, -0.2095,  ...,  0.0658, -0.1989,  0.7932],\n",
      "          [ 1.7548,  1.1505,  1.1179,  ...,  1.0847,  0.6095,  1.4771],\n",
      "          [ 2.2458,  0.8914,  0.5037,  ...,  0.7660,  0.8200,  1.4358],\n",
      "          ...,\n",
      "          [ 1.4964,  0.4584,  0.8676,  ...,  0.8732,  0.5091,  1.4981],\n",
      "          [ 1.9000,  1.2638,  0.9650,  ...,  1.0450,  0.5896,  1.4537],\n",
      "          [ 1.8078,  1.2235,  1.3164,  ...,  1.5284,  1.2299,  2.0208]],\n",
      "\n",
      "         [[-0.2474, -0.2246, -0.2092,  ..., -0.2297, -0.1943, -0.2467],\n",
      "          [-0.2463, -0.2471, -0.2516,  ..., -0.2206, -0.2151, -0.2644],\n",
      "          [-0.2313, -0.2181, -0.2379,  ..., -0.2187, -0.2165, -0.2551],\n",
      "          ...,\n",
      "          [-0.2552, -0.2593, -0.2155,  ..., -0.2208, -0.2264, -0.2764],\n",
      "          [-0.2142, -0.2350, -0.2238,  ..., -0.2208, -0.2171, -0.2705],\n",
      "          [-0.2785, -0.2678, -0.2750,  ..., -0.2734, -0.2762, -0.2598]],\n",
      "\n",
      "         [[-0.1584, -0.1628, -0.1050,  ..., -0.1283, -0.1359, -0.1961],\n",
      "          [-0.1478, -0.0959, -0.1222,  ..., -0.1457, -0.1478, -0.1407],\n",
      "          [-0.1345, -0.1261, -0.1313,  ..., -0.1234, -0.1502, -0.2060],\n",
      "          ...,\n",
      "          [-0.1581, -0.1566, -0.1214,  ..., -0.1227, -0.1332, -0.1473],\n",
      "          [-0.1630, -0.1034, -0.1435,  ..., -0.1499, -0.1373, -0.1404],\n",
      "          [-0.1654, -0.1633, -0.1271,  ..., -0.1168, -0.1383, -0.1937]]]],\n",
      "       grad_fn=<SiluBackward0>)\n",
      "<built-in method type of Tensor object at 0x000001784DAC54F0>\n",
      "tensor([[[[-1.7937e-01, -9.2510e-02, -1.0559e-01,  ..., -8.9744e-02,\n",
      "           -1.0047e-01, -4.2880e-02],\n",
      "          [-8.7721e-02, -4.5002e-02, -5.0754e-02,  ..., -5.6171e-02,\n",
      "           -4.9900e-02,  5.2434e-03],\n",
      "          [-1.1078e-01, -8.0764e-02, -8.4922e-02,  ..., -7.6282e-02,\n",
      "           -7.0042e-02, -6.0268e-02],\n",
      "          ...,\n",
      "          [-1.2504e-01, -9.2107e-02, -8.8562e-02,  ..., -1.4102e-02,\n",
      "           -9.9633e-02, -2.3798e-02],\n",
      "          [-1.2670e-01, -5.4069e-02, -7.1213e-02,  ..., -1.4814e-02,\n",
      "           -8.6611e-02, -5.7999e-02],\n",
      "          [-4.9803e-02, -2.6686e-02,  2.0911e-02,  ..., -3.9517e-02,\n",
      "           -4.7388e-04,  2.4059e-02]],\n",
      "\n",
      "         [[-2.5938e-01, -2.7727e-01, -2.7352e-01,  ..., -2.7732e-01,\n",
      "           -2.7261e-01, -2.7561e-01],\n",
      "          [-2.7483e-01, -2.7845e-01, -2.4766e-01,  ..., -2.5789e-01,\n",
      "           -2.5546e-01, -2.7755e-01],\n",
      "          [-2.7529e-01, -2.6863e-01, -2.6404e-01,  ..., -2.3780e-01,\n",
      "           -2.4148e-01, -2.6866e-01],\n",
      "          ...,\n",
      "          [-2.7717e-01, -2.7717e-01, -2.4653e-01,  ..., -2.4845e-01,\n",
      "           -2.6498e-01, -2.7605e-01],\n",
      "          [-2.7755e-01, -2.6668e-01, -2.7846e-01,  ..., -2.7846e-01,\n",
      "           -2.4938e-01, -2.6539e-01],\n",
      "          [-1.9618e-01, -2.4001e-01, -2.7808e-01,  ..., -2.4977e-01,\n",
      "           -2.7113e-01, -2.7774e-01]],\n",
      "\n",
      "         [[ 6.8131e-01, -1.5434e-01, -2.1204e-01,  ..., -1.5085e-01,\n",
      "           -1.1302e-01, -2.7845e-01],\n",
      "          [-2.1049e-01, -2.7475e-01, -2.2296e-01,  ..., -2.0787e-01,\n",
      "           -2.7405e-01, -2.5779e-01],\n",
      "          [-2.7484e-01, -1.7971e-01, -2.7757e-01,  ..., -2.0331e-01,\n",
      "           -2.3290e-01, -2.5533e-01],\n",
      "          ...,\n",
      "          [-1.5045e-01, -2.7318e-01, -2.3568e-01,  ..., -2.6733e-01,\n",
      "           -2.6291e-01, -2.3740e-01],\n",
      "          [-1.8264e-01, -2.3837e-01, -2.2183e-01,  ..., -2.7846e-01,\n",
      "           -2.6088e-01, -2.7760e-01],\n",
      "          [-2.3037e-01, -2.2939e-01, -2.6845e-01,  ..., -2.7577e-01,\n",
      "           -2.7735e-01, -2.7569e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.8565e-01,  5.5604e-01,  3.2368e-01,  ...,  5.1545e-01,\n",
      "            4.5090e-01,  1.8426e-01],\n",
      "          [ 5.1670e-01,  2.7126e-01,  6.7391e-01,  ...,  4.6144e-01,\n",
      "            4.6847e-01, -1.0166e-02],\n",
      "          [ 4.5167e-01,  1.9426e-01,  7.3206e-01,  ...,  2.3372e-01,\n",
      "            1.5101e-01,  2.4469e-01],\n",
      "          ...,\n",
      "          [ 6.7149e-01,  3.0968e-01, -5.0810e-02,  ...,  6.9461e-01,\n",
      "            2.9067e-01,  1.3601e-01],\n",
      "          [ 5.6520e-01,  3.9965e-01,  8.9567e-02,  ...,  7.9165e-01,\n",
      "            8.6530e-01,  1.5282e-01],\n",
      "          [ 5.9769e-01,  3.3852e-01,  1.4812e-01,  ...,  3.9832e-01,\n",
      "            2.6744e-01,  1.0023e-01]],\n",
      "\n",
      "         [[-1.4407e-01, -9.0814e-02, -1.5316e-01,  ..., -1.9807e-01,\n",
      "           -1.7660e-01, -2.0680e-01],\n",
      "          [-2.3386e-01, -2.4486e-01, -2.5017e-01,  ..., -2.6021e-01,\n",
      "           -2.4555e-01, -2.6719e-01],\n",
      "          [-2.4116e-01, -2.4668e-01, -2.1584e-01,  ..., -2.3028e-01,\n",
      "           -2.0440e-01, -1.9594e-01],\n",
      "          ...,\n",
      "          [-1.9886e-01, -2.3477e-01, -2.6837e-01,  ..., -1.2974e-01,\n",
      "           -1.7891e-01, -1.7912e-01],\n",
      "          [-2.0987e-01, -2.6667e-01, -2.3969e-01,  ..., -1.3033e-01,\n",
      "           -1.4602e-01, -1.5420e-01],\n",
      "          [-1.7689e-01, -2.3491e-01, -1.9248e-01,  ..., -5.2822e-02,\n",
      "           -2.5885e-02, -7.4082e-02]],\n",
      "\n",
      "         [[ 4.5865e-01,  1.8518e-01,  9.3829e-02,  ...,  1.4100e-01,\n",
      "            1.5789e-01,  2.4447e-01],\n",
      "          [ 3.5137e-01,  1.0019e-01,  1.4254e-01,  ...,  3.5254e-02,\n",
      "            9.9750e-02,  1.3369e-01],\n",
      "          [ 3.2383e-01,  6.4591e-02,  1.6316e-01,  ...,  4.8065e-02,\n",
      "            2.9725e-02,  1.0987e-01],\n",
      "          ...,\n",
      "          [ 3.8584e-01,  1.1496e-01,  3.1390e-02,  ...,  1.1173e-01,\n",
      "            7.7055e-02,  9.9827e-02],\n",
      "          [ 3.8050e-01,  1.2020e-01,  4.2241e-02,  ...,  1.1966e-01,\n",
      "            6.1009e-02,  1.4789e-01],\n",
      "          [ 3.4608e-01,  7.3298e-02,  5.0650e-02,  ...,  1.2457e-01,\n",
      "            9.3267e-02,  6.0475e-02]]]], grad_fn=<SiluBackward0>)\n",
      "<built-in method type of Tensor object at 0x000001784DAC5950>\n",
      "tensor([[[[ 2.3556e-01,  1.2989e+00,  2.4189e+00,  ...,  1.8121e+00,\n",
      "            1.1501e+00,  1.2398e+00],\n",
      "          [ 2.4863e-01,  1.7838e+00,  1.1217e+00,  ...,  2.0988e+00,\n",
      "            1.4968e+00,  2.1527e+00],\n",
      "          [ 4.1992e-02,  1.2143e+00,  1.0553e+00,  ...,  1.1118e+00,\n",
      "            1.3944e+00,  2.5141e+00],\n",
      "          ...,\n",
      "          [-2.2523e-01,  9.9245e-01,  2.4935e+00,  ...,  1.7426e+00,\n",
      "            1.9020e+00,  2.1254e+00],\n",
      "          [-1.4574e-01,  1.5057e+00,  1.9870e+00,  ...,  1.3252e+00,\n",
      "            1.7232e+00,  2.7187e+00],\n",
      "          [ 6.5030e-01,  2.2673e+00,  2.8778e+00,  ...,  2.3650e+00,\n",
      "            2.9992e+00,  3.8776e+00]],\n",
      "\n",
      "         [[ 5.1200e-02,  3.7150e-02,  6.6268e-02,  ...,  1.7277e-01,\n",
      "            7.7201e-02,  7.7987e-02],\n",
      "          [ 1.2170e-02,  4.6282e-02, -3.2667e-03,  ...,  1.8675e-01,\n",
      "            3.9584e-02,  7.8015e-02],\n",
      "          [ 1.9284e-02,  6.8155e-02,  2.1236e-02,  ...,  2.3212e-02,\n",
      "           -2.2391e-02,  6.1882e-02],\n",
      "          ...,\n",
      "          [-6.5498e-02,  5.1613e-02,  6.4010e-02,  ...,  1.2873e-01,\n",
      "            1.1119e-01,  8.7785e-02],\n",
      "          [-8.7909e-02,  1.3149e-01,  8.1010e-02,  ...,  4.9299e-02,\n",
      "            8.6918e-02,  1.1453e-01],\n",
      "          [ 5.9302e-02,  1.1368e-01,  1.1569e-01,  ...,  1.0429e-01,\n",
      "            1.3808e-01,  8.4568e-02]],\n",
      "\n",
      "         [[ 3.1304e-01,  8.0419e-01,  5.0720e-01,  ...,  3.4349e-01,\n",
      "            3.1377e-01,  1.1975e+00],\n",
      "          [ 3.4803e-01,  5.5610e-01,  4.6684e-01,  ...,  3.7734e-01,\n",
      "            5.0393e-01,  1.0174e+00],\n",
      "          [ 2.0928e-01,  6.4808e-01,  3.5769e-01,  ...,  4.7216e-01,\n",
      "            5.7124e-01,  7.6012e-01],\n",
      "          ...,\n",
      "          [ 4.5386e-01,  4.4061e-01,  3.9350e-01,  ...,  5.4135e-01,\n",
      "            3.9599e-01,  1.0336e+00],\n",
      "          [ 4.5767e-01,  5.1879e-01,  4.7273e-01,  ...,  6.2703e-01,\n",
      "            7.5251e-01,  7.1717e-01],\n",
      "          [ 1.2286e+00,  9.8502e-01,  1.1696e+00,  ...,  1.0769e+00,\n",
      "            1.3580e+00,  1.9613e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.6129e-01,  1.2201e+00,  1.6735e+00,  ...,  1.5622e+00,\n",
      "            1.0986e+00,  1.2286e+00],\n",
      "          [ 1.0154e+00,  1.1050e+00,  1.7081e+00,  ...,  2.5002e+00,\n",
      "            2.1513e+00,  1.6635e+00],\n",
      "          [ 8.8288e-01,  2.0678e+00,  2.2481e+00,  ...,  2.7212e+00,\n",
      "            2.2593e+00,  1.9211e+00],\n",
      "          ...,\n",
      "          [ 7.2786e-01,  1.2762e+00,  2.2475e+00,  ...,  2.4676e+00,\n",
      "            2.1702e+00,  1.8353e+00],\n",
      "          [ 6.6845e-01,  1.4328e+00,  2.4576e+00,  ...,  1.9931e+00,\n",
      "            2.1720e+00,  1.2868e+00],\n",
      "          [ 7.5512e-01,  1.1545e+00,  1.6672e+00,  ...,  1.6931e+00,\n",
      "            1.7672e+00,  1.6704e+00]],\n",
      "\n",
      "         [[ 2.9059e-02,  5.3609e-02, -5.3742e-02,  ...,  1.2974e-02,\n",
      "           -2.2397e-02,  6.5262e-02],\n",
      "          [-2.4822e-01, -2.4847e-01, -2.5200e-01,  ..., -2.6915e-01,\n",
      "           -2.7242e-01, -1.8698e-01],\n",
      "          [-2.4549e-01, -2.6048e-01, -2.3411e-01,  ..., -2.1874e-01,\n",
      "           -2.3670e-01, -1.4571e-01],\n",
      "          ...,\n",
      "          [-1.4287e-01, -2.0261e-01, -2.5751e-01,  ..., -2.3701e-01,\n",
      "           -2.0976e-01, -9.7750e-02],\n",
      "          [-1.4742e-01, -1.9335e-01, -1.9136e-01,  ..., -1.7002e-01,\n",
      "           -2.3190e-01, -3.2717e-02],\n",
      "          [ 9.7350e-02, -1.7181e-03, -8.6479e-03,  ..., -4.5726e-03,\n",
      "            4.9420e-02,  1.4085e-01]],\n",
      "\n",
      "         [[-2.2456e-01, -2.6816e-01, -2.7800e-01,  ..., -2.7204e-01,\n",
      "           -2.6895e-01, -2.0702e-01],\n",
      "          [-2.6559e-01, -2.7832e-01, -2.7703e-01,  ..., -2.6155e-01,\n",
      "           -2.7782e-01, -2.5271e-01],\n",
      "          [-2.4643e-01, -2.7828e-01, -2.7335e-01,  ..., -2.6111e-01,\n",
      "           -2.7106e-01, -2.6779e-01],\n",
      "          ...,\n",
      "          [-2.4079e-01, -2.5667e-01, -2.7751e-01,  ..., -2.7563e-01,\n",
      "           -2.7185e-01, -2.7426e-01],\n",
      "          [-2.7037e-01, -2.7050e-01, -2.6178e-01,  ..., -2.4900e-01,\n",
      "           -2.7674e-01, -2.3654e-01],\n",
      "          [-2.3006e-01, -2.4984e-01, -2.4663e-01,  ..., -2.7718e-01,\n",
      "           -2.7588e-01, -2.4703e-01]]]], grad_fn=<SiluBackward0>)\n",
      "<built-in method type of Tensor object at 0x000001784DAC5AE0>\n",
      "tensor([[[[ 2.0694e-01,  1.7239e-01,  1.9958e-01,  ...,  2.5377e-01,\n",
      "            1.7534e-01,  2.0791e-01],\n",
      "          [ 1.4194e-01, -5.1715e-03,  4.2593e-02,  ...,  7.1597e-02,\n",
      "            8.5579e-02,  1.6229e-01],\n",
      "          [ 1.5739e-01, -9.8805e-03, -2.7155e-02,  ...,  6.0051e-02,\n",
      "            7.2070e-02,  6.3125e-02],\n",
      "          ...,\n",
      "          [ 1.1898e-01,  1.8757e-02,  1.0569e-01,  ...,  6.3173e-02,\n",
      "            5.6601e-02,  7.7332e-02],\n",
      "          [ 1.7598e-01,  4.5336e-02,  1.0918e-01,  ...,  1.4355e-02,\n",
      "            4.7115e-02,  8.5382e-03],\n",
      "          [ 6.7244e-02,  4.7949e-02,  4.9069e-02,  ...,  1.0587e-02,\n",
      "            4.2914e-03,  5.5084e-02]],\n",
      "\n",
      "         [[ 6.1096e-03,  3.0328e-02, -3.9354e-02,  ..., -7.0748e-02,\n",
      "           -6.1911e-02,  1.2850e-02],\n",
      "          [-5.5872e-02, -8.0365e-02, -5.7775e-02,  ..., -1.4906e-01,\n",
      "           -5.2716e-02, -2.0618e-04],\n",
      "          [-4.1657e-02, -1.1029e-01, -7.3086e-02,  ..., -1.1616e-01,\n",
      "           -1.0898e-01,  2.0405e-02],\n",
      "          ...,\n",
      "          [ 4.8808e-03, -1.0150e-01, -1.1111e-01,  ...,  1.6214e-02,\n",
      "           -4.1001e-02,  2.4213e-02],\n",
      "          [-8.9346e-03, -1.1955e-01, -1.2678e-01,  ..., -6.7300e-03,\n",
      "           -1.0943e-01,  1.7783e-02],\n",
      "          [-2.1606e-02, -3.6387e-02, -3.0680e-02,  ..., -1.8407e-02,\n",
      "           -3.8517e-02,  1.0559e-01]],\n",
      "\n",
      "         [[-6.2524e-03,  2.4490e-01,  6.2304e-01,  ...,  2.8826e-01,\n",
      "            9.5044e-02,  7.7639e-01],\n",
      "          [ 3.6370e-01, -1.7183e-01,  1.2565e-02,  ...,  5.3602e-01,\n",
      "            2.0866e-01,  7.4656e-01],\n",
      "          [ 6.4570e-01,  2.4351e-01,  6.1679e-01,  ...,  1.0000e+00,\n",
      "            7.5399e-01,  1.2925e+00],\n",
      "          ...,\n",
      "          [ 6.1459e-01,  1.6651e-01,  8.6974e-01,  ...,  6.1174e-01,\n",
      "            6.7239e-01,  1.3497e+00],\n",
      "          [ 6.9959e-01,  1.3730e-01,  6.3547e-01,  ...,  7.3349e-01,\n",
      "            6.4634e-01,  9.8889e-01],\n",
      "          [ 7.5919e-01,  6.6198e-01,  5.9358e-01,  ...,  7.9142e-01,\n",
      "            7.7750e-01,  1.5967e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.4086e-01, -2.7716e-01, -2.7839e-01,  ..., -2.5736e-01,\n",
      "           -2.7828e-01, -2.7551e-01],\n",
      "          [-1.8199e-01, -2.1061e-01, -2.3202e-01,  ..., -2.1068e-01,\n",
      "           -1.9284e-01, -2.0919e-01],\n",
      "          [-1.0380e-01, -1.9025e-01, -2.3864e-01,  ..., -1.8692e-01,\n",
      "           -1.9370e-01, -2.6501e-01],\n",
      "          ...,\n",
      "          [-1.4871e-02, -2.2192e-01, -1.9915e-01,  ..., -2.7818e-01,\n",
      "           -2.4127e-01, -2.3737e-01],\n",
      "          [ 1.4959e-01, -2.0422e-01, -1.7812e-01,  ..., -2.6497e-01,\n",
      "           -2.1364e-01, -1.9073e-01],\n",
      "          [-2.6905e-01, -2.0089e-01, -2.1414e-01,  ..., -2.0118e-01,\n",
      "           -1.6355e-01, -2.7325e-01]],\n",
      "\n",
      "         [[-7.2748e-03,  1.1344e-02,  1.0467e-01,  ...,  1.0934e-01,\n",
      "            1.2270e-01,  1.2533e-01],\n",
      "          [-6.6939e-02,  1.4142e-01,  1.5300e-01,  ..., -3.2475e-02,\n",
      "            7.2864e-02, -4.5330e-02],\n",
      "          [-1.1384e-01,  8.7557e-03,  3.6812e-02,  ...,  4.9503e-02,\n",
      "            1.6589e-02,  5.5700e-02],\n",
      "          ...,\n",
      "          [ 2.1680e-02,  6.7547e-02, -2.7213e-02,  ...,  3.4695e-01,\n",
      "            1.6438e-01,  1.4960e-01],\n",
      "          [-8.2845e-02, -8.4454e-02,  2.6128e-02,  ...,  1.5621e-01,\n",
      "            1.0392e-01,  2.2755e-01],\n",
      "          [ 5.6162e-04,  1.4500e-02,  1.1831e-01,  ...,  2.1390e-02,\n",
      "            1.4145e-01,  3.0551e-01]],\n",
      "\n",
      "         [[ 2.7284e+00,  3.4409e-01,  6.2077e-01,  ...,  9.3119e-01,\n",
      "            9.4863e-01,  1.0365e+00],\n",
      "          [ 8.3326e-01, -2.1417e-01,  1.5555e-01,  ...,  6.8664e-02,\n",
      "            9.6184e-02,  6.2621e-01],\n",
      "          [ 1.1663e+00, -2.5366e-01, -1.4225e-01,  ...,  1.8572e-02,\n",
      "           -6.9647e-02,  5.4858e-01],\n",
      "          ...,\n",
      "          [ 8.7216e-01, -2.6960e-01,  2.5112e-02,  ..., -8.4547e-03,\n",
      "            6.1757e-03,  1.4676e-01],\n",
      "          [ 7.3204e-01, -2.3079e-01, -2.1914e-02,  ..., -2.5670e-01,\n",
      "           -2.0965e-01,  1.7926e-01],\n",
      "          [-1.5096e-01, -6.1289e-02,  3.1030e-01,  ..., -9.8149e-02,\n",
      "           -1.5807e-01,  2.8793e-01]]]], grad_fn=<SiluBackward0>)\n",
      "<built-in method type of Tensor object at 0x000001784DAC1270>\n",
      "tensor([[[[ 8.2736e-01,  9.2060e-01,  5.4449e-01,  ...,  5.3341e-01,\n",
      "            6.3838e-01,  4.2236e-01],\n",
      "          [ 3.3153e-01,  9.6054e-01,  1.5159e+00,  ...,  7.4724e-01,\n",
      "            1.7258e+00,  3.9099e-01],\n",
      "          [-2.7239e-01,  3.6011e-01,  6.6001e-01,  ...,  7.5062e-01,\n",
      "            5.2971e-01,  1.3377e-01],\n",
      "          ...,\n",
      "          [-2.3570e-01,  6.7963e-01,  6.8033e-01,  ...,  2.2363e+00,\n",
      "            1.0337e+00, -1.8986e-02],\n",
      "          [ 1.5614e-01,  5.0452e-01,  1.3004e+00,  ...,  1.7991e+00,\n",
      "            8.2300e-01,  5.8936e-01],\n",
      "          [-1.1920e-01,  8.1662e-01,  1.9175e+00,  ...,  1.5108e+00,\n",
      "            4.0051e-01,  3.0299e-01]],\n",
      "\n",
      "         [[-1.1265e-01, -1.9480e-01, -1.1657e-01,  ..., -1.3053e-01,\n",
      "           -1.0966e-01, -9.5027e-02],\n",
      "          [-8.3123e-02, -2.7730e-01, -2.6650e-01,  ..., -2.7039e-01,\n",
      "           -2.2718e-01, -1.3979e-01],\n",
      "          [-1.2251e-01, -2.4672e-01, -2.1630e-01,  ..., -2.1190e-01,\n",
      "           -2.7345e-01, -2.1891e-01],\n",
      "          ...,\n",
      "          [-1.4819e-01, -7.9526e-02, -1.1936e-01,  ..., -2.6828e-01,\n",
      "           -2.6657e-01, -2.6612e-01],\n",
      "          [-1.5040e-01, -1.6184e-01, -4.2230e-02,  ..., -1.8047e-01,\n",
      "           -2.5465e-01, -2.7694e-01],\n",
      "          [-1.3728e-01, -2.6356e-01, -1.5640e-01,  ..., -2.5065e-01,\n",
      "           -1.8793e-01, -2.5172e-01]],\n",
      "\n",
      "         [[-2.0249e-01, -1.4927e-01,  1.8306e-01,  ..., -1.2507e-01,\n",
      "           -1.3772e-01, -9.1834e-02],\n",
      "          [-2.7741e-01, -2.7761e-01, -2.3709e-01,  ..., -1.9671e-01,\n",
      "           -6.8304e-02, -1.9974e-01],\n",
      "          [-2.7746e-01, -2.6373e-01, -1.3848e-01,  ..., -2.6080e-01,\n",
      "           -2.5880e-01, -2.5216e-01],\n",
      "          ...,\n",
      "          [-2.3641e-01, -1.6493e-01, -2.7637e-01,  ..., -2.7724e-01,\n",
      "           -2.7761e-01, -2.6067e-01],\n",
      "          [-1.9806e-01, -1.9265e-01, -2.7625e-01,  ..., -2.4923e-01,\n",
      "           -2.5893e-01, -2.7586e-01],\n",
      "          [-2.6955e-01, -2.6224e-01, -2.5418e-01,  ..., -2.7747e-01,\n",
      "           -2.6465e-01, -2.5378e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2853e+00, -1.3197e-01,  5.4924e-01,  ...,  4.3910e-01,\n",
      "            8.6624e-01,  7.6405e-01],\n",
      "          [-8.8772e-02,  1.8798e-01,  4.2863e-01,  ...,  3.0888e-01,\n",
      "            7.3474e-03, -6.1286e-02],\n",
      "          [-2.0248e-01,  9.7025e-02,  1.0198e+00,  ...,  9.5991e-01,\n",
      "            1.7126e-01,  6.1493e-01],\n",
      "          ...,\n",
      "          [ 2.4239e-02,  4.6249e-01,  1.2870e-01,  ...,  1.3449e+00,\n",
      "            6.7801e-01,  1.3635e-01],\n",
      "          [-4.4778e-02, -8.1890e-02,  4.8717e-01,  ...,  9.3361e-01,\n",
      "           -1.4961e-03,  5.2869e-01],\n",
      "          [-2.1397e-01,  9.9259e-02, -2.4217e-02,  ...,  4.3555e-01,\n",
      "            2.3078e-03, -8.3768e-02]],\n",
      "\n",
      "         [[-1.9307e-01,  3.5516e-01,  2.0553e-01,  ...,  1.5012e-01,\n",
      "            1.0105e+00,  1.6573e+00],\n",
      "          [ 4.2136e-01,  8.8361e-01,  1.2164e+00,  ...,  7.1133e-01,\n",
      "            1.6098e+00,  2.0590e+00],\n",
      "          [ 2.6215e-02,  8.2169e-01,  8.1622e-01,  ...,  8.4999e-01,\n",
      "            9.1259e-01,  1.2822e+00],\n",
      "          ...,\n",
      "          [ 2.4218e-01,  6.2511e-01,  5.6004e-01,  ...,  1.0521e+00,\n",
      "            5.9410e-01,  5.4381e-01],\n",
      "          [ 8.1238e-01,  1.0457e+00,  1.2333e+00,  ...,  1.6919e+00,\n",
      "            9.4555e-01,  1.1555e+00],\n",
      "          [ 2.1047e+00,  1.5108e+00,  2.4111e+00,  ...,  5.1613e-01,\n",
      "            1.5800e+00,  2.3888e+00]],\n",
      "\n",
      "         [[-2.3585e-01, -2.6666e-01, -2.5923e-01,  ..., -2.6018e-01,\n",
      "           -2.5162e-01, -1.9618e-01],\n",
      "          [-2.7804e-01, -2.5882e-01, -2.7066e-01,  ..., -2.7399e-01,\n",
      "           -2.7839e-01, -2.6879e-01],\n",
      "          [-2.4445e-01, -2.5747e-01, -2.7457e-01,  ..., -2.7118e-01,\n",
      "           -2.7180e-01, -2.6282e-01],\n",
      "          ...,\n",
      "          [-2.3296e-01, -2.5150e-01, -2.7317e-01,  ..., -2.7821e-01,\n",
      "           -2.5969e-01, -2.7184e-01],\n",
      "          [-2.7302e-01, -2.3778e-01, -2.6349e-01,  ..., -2.5662e-01,\n",
      "           -2.6313e-01, -2.4966e-01],\n",
      "          [-2.3787e-01, -2.5135e-01, -2.6444e-01,  ..., -2.3823e-01,\n",
      "           -2.2809e-01, -2.5101e-01]]]], grad_fn=<SiluBackward0>)\n",
      "<built-in method type of Tensor object at 0x000001784DAC16D0>\n",
      "tensor([[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.2203, -0.2566,  ..., -0.2782,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.1373, -0.2455,  ..., -0.2377,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000, -0.1852, -0.2588,  ..., -0.2632,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.1112, -0.2596,  ..., -0.2569,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.2379, -0.0999,  ..., -0.2545,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000, -0.2141, -0.2778,  ..., -0.2738,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.1191, -0.1587,  ..., -0.1524,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.2435, -0.2663,  ..., -0.2527,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000, -0.1963, -0.1855,  ..., -0.2769,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.1035,  0.0240,  ...,  0.3663,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.1789, -0.1539,  ...,  0.0478,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.1847, -0.0399,  ...,  0.8085,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.1230,  0.0431,  ..., -0.1877,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.1142, -0.2330,  ..., -0.2405,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000, -0.1923, -0.2657,  ..., -0.2733,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.1740, -0.2596,  ..., -0.2425,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.1469, -0.2781,  ..., -0.1478,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000, -0.1303, -0.1342,  ..., -0.1249,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],\n",
      "       grad_fn=<ConstantPadNdBackward0>)\n",
      "<built-in method type of Tensor object at 0x000001784DAC1CC0>\n",
      "tensor([[[[ 1.2799e-01, -2.4021e-01, -2.7336e-01,  ..., -2.7718e-01,\n",
      "           -2.6271e-01, -1.5479e-01],\n",
      "          [-3.4354e-02, -2.1636e-01, -2.5971e-01,  ..., -2.6015e-01,\n",
      "           -2.0389e-01, -4.0331e-02],\n",
      "          [ 2.4934e-02, -1.0226e-01, -2.3254e-01,  ..., -2.4251e-01,\n",
      "           -1.8433e-01, -8.2335e-02],\n",
      "          ...,\n",
      "          [ 9.6714e-02, -1.4517e-01, -2.1026e-01,  ..., -2.5599e-01,\n",
      "           -2.3002e-01, -9.0566e-02],\n",
      "          [ 3.7876e-02, -1.5511e-01, -2.3114e-01,  ..., -2.5404e-01,\n",
      "           -2.1470e-01, -2.9788e-02],\n",
      "          [ 1.5791e-01, -2.5401e-01, -2.7177e-01,  ..., -2.7663e-01,\n",
      "           -1.6963e-01, -2.3964e-01]],\n",
      "\n",
      "         [[ 1.3835e-03,  2.8952e-02,  1.6068e-02,  ..., -1.5291e-03,\n",
      "           -8.5100e-03, -1.3445e-02],\n",
      "          [-2.7581e-02, -7.1681e-03, -1.1415e-02,  ..., -4.9077e-02,\n",
      "           -4.7652e-02, -1.1132e-02],\n",
      "          [-5.7442e-02, -2.7273e-02,  1.1496e-02,  ..., -9.3663e-03,\n",
      "           -4.3407e-02, -4.6670e-02],\n",
      "          ...,\n",
      "          [-5.3204e-02, -2.6306e-02, -6.8868e-03,  ..., -1.0703e-02,\n",
      "           -4.5219e-02, -4.6518e-02],\n",
      "          [-1.7836e-02, -5.5701e-03, -1.7490e-02,  ...,  2.2783e-03,\n",
      "           -4.7100e-02, -3.8847e-02],\n",
      "          [-3.9918e-03, -9.1458e-03,  6.4640e-03,  ...,  1.1808e-02,\n",
      "           -2.0393e-02,  5.1136e-02]],\n",
      "\n",
      "         [[ 1.8180e+00,  4.8661e-01,  4.4598e-01,  ...,  5.1675e-01,\n",
      "            6.2853e-01,  7.6211e-01],\n",
      "          [ 7.1442e-01, -2.6802e-01, -2.7683e-01,  ..., -2.7842e-01,\n",
      "           -2.7833e-01, -8.3112e-02],\n",
      "          [ 3.0408e-01, -2.7576e-01, -2.6162e-01,  ..., -2.7536e-01,\n",
      "           -2.7708e-01, -1.9836e-01],\n",
      "          ...,\n",
      "          [ 3.4563e-01, -2.7666e-01, -2.6698e-01,  ..., -2.7841e-01,\n",
      "           -2.7828e-01, -2.3672e-01],\n",
      "          [ 8.2699e-01, -2.5064e-01, -2.7801e-01,  ..., -2.7794e-01,\n",
      "           -2.7162e-01, -2.5695e-01],\n",
      "          [ 9.7504e-01,  1.6254e-01, -5.6319e-02,  ...,  1.1470e-01,\n",
      "            2.0754e-01,  9.8600e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.2660e-01, -2.6486e-01, -2.7383e-01,  ..., -2.6715e-01,\n",
      "           -2.6651e-01, -2.6612e-01],\n",
      "          [-2.2493e-01, -1.8000e-01, -2.2687e-01,  ..., -2.2817e-01,\n",
      "           -2.2739e-01, -2.0227e-01],\n",
      "          [-2.4255e-01, -1.4866e-01, -1.1817e-01,  ..., -1.8788e-01,\n",
      "           -1.9129e-01, -1.8508e-01],\n",
      "          ...,\n",
      "          [-2.4935e-01, -1.3582e-01, -5.5684e-02,  ..., -1.4674e-01,\n",
      "           -1.4171e-01, -1.5737e-01],\n",
      "          [-1.6023e-01, -8.5500e-02, -1.3641e-01,  ..., -1.6076e-01,\n",
      "           -6.3104e-02, -1.0893e-01],\n",
      "          [-7.7543e-02, -1.6898e-01, -2.4472e-01,  ..., -2.2819e-01,\n",
      "           -1.7500e-01, -1.8932e-01]],\n",
      "\n",
      "         [[-2.6429e-01, -2.7380e-01, -2.7495e-01,  ..., -2.7631e-01,\n",
      "           -2.7535e-01, -2.7721e-01],\n",
      "          [-2.7789e-01, -2.5763e-01, -2.7139e-01,  ..., -2.7842e-01,\n",
      "           -2.7840e-01, -2.7769e-01],\n",
      "          [-2.7674e-01, -2.7689e-01, -2.7764e-01,  ..., -2.7679e-01,\n",
      "           -2.7794e-01, -2.7846e-01],\n",
      "          ...,\n",
      "          [-2.7641e-01, -2.7820e-01, -2.7690e-01,  ..., -2.7396e-01,\n",
      "           -2.7799e-01, -2.7832e-01],\n",
      "          [-2.7692e-01, -2.7793e-01, -2.7815e-01,  ..., -2.7477e-01,\n",
      "           -2.7840e-01, -2.7702e-01],\n",
      "          [-2.7139e-01, -2.7818e-01, -2.7432e-01,  ..., -2.7541e-01,\n",
      "           -2.7694e-01, -2.5376e-01]],\n",
      "\n",
      "         [[-9.7088e-02, -2.9635e-02,  1.5987e-05,  ...,  1.3394e-01,\n",
      "           -7.6993e-02, -7.6143e-02],\n",
      "          [-5.5456e-02, -1.2959e-01, -1.4260e-01,  ..., -1.2104e-01,\n",
      "           -1.8177e-01, -1.4859e-01],\n",
      "          [-1.2736e-01, -1.7528e-01, -1.2539e-01,  ..., -1.1472e-01,\n",
      "           -1.6409e-01, -8.4934e-02],\n",
      "          ...,\n",
      "          [-1.0509e-01, -1.8700e-01, -1.4383e-01,  ..., -9.5713e-02,\n",
      "           -1.9309e-01, -1.5229e-01],\n",
      "          [-1.2005e-01, -1.9079e-01, -1.5351e-01,  ..., -1.1590e-01,\n",
      "           -1.7490e-01, -1.4103e-01],\n",
      "          [-4.1290e-02, -1.2021e-01, -3.9371e-02,  ...,  1.6274e-02,\n",
      "           -8.6304e-02, -3.4431e-02]]]], grad_fn=<SiluBackward0>)\n",
      "<built-in method type of Tensor object at 0x000001784DAC2220>\n",
      "tensor([[[[ 0.1159,  0.0118,  0.0012,  ..., -0.0105,  0.0121,  0.0737],\n",
      "          [ 0.0754,  0.0211,  0.0088,  ..., -0.0044,  0.0237,  0.0729],\n",
      "          [ 0.0221,  0.0305,  0.0414,  ...,  0.0327,  0.0383,  0.0167],\n",
      "          ...,\n",
      "          [ 0.0100,  0.0028,  0.0226,  ...,  0.0068, -0.0010,  0.0259],\n",
      "          [ 0.0124,  0.0058,  0.0114,  ...,  0.0210, -0.0094,  0.0120],\n",
      "          [-0.0235, -0.0528, -0.0286,  ..., -0.0356, -0.0471, -0.0613]],\n",
      "\n",
      "         [[-0.2434, -0.2246, -0.2422,  ..., -0.2439, -0.2406, -0.1992],\n",
      "          [-0.1510, -0.1872, -0.1934,  ..., -0.2465, -0.2385, -0.1697],\n",
      "          [-0.1662, -0.2368, -0.2552,  ..., -0.2649, -0.2669, -0.1719],\n",
      "          ...,\n",
      "          [-0.1419, -0.2374, -0.2653,  ..., -0.2697, -0.2360, -0.2250],\n",
      "          [-0.1592, -0.2212, -0.2469,  ..., -0.2748, -0.2637, -0.2113],\n",
      "          [-0.2255, -0.2131, -0.2462,  ..., -0.2576, -0.2757, -0.1369]],\n",
      "\n",
      "         [[-0.2724, -0.2408, -0.2245,  ..., -0.2170, -0.2292, -0.2615],\n",
      "          [-0.2295, -0.2784, -0.2758,  ..., -0.2549, -0.2741, -0.2740],\n",
      "          [-0.2325, -0.2685, -0.2729,  ..., -0.2729, -0.2780, -0.2761],\n",
      "          ...,\n",
      "          [-0.2111, -0.2640, -0.2785,  ..., -0.2768, -0.2785, -0.2771],\n",
      "          [-0.2472, -0.2770, -0.2702,  ..., -0.2678, -0.2781, -0.2764],\n",
      "          [-0.2625, -0.2286, -0.2325,  ..., -0.2152, -0.2466, -0.2510]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2378, -0.2080, -0.1599,  ..., -0.2047, -0.1808, -0.2212],\n",
      "          [-0.2411, -0.2589, -0.2555,  ..., -0.2552, -0.2548, -0.2783],\n",
      "          [-0.2294, -0.2689, -0.2710,  ..., -0.2783, -0.2579, -0.2783],\n",
      "          ...,\n",
      "          [-0.2347, -0.2765, -0.2629,  ..., -0.2711, -0.2300, -0.2759],\n",
      "          [-0.2551, -0.2666, -0.2450,  ..., -0.2512, -0.2440, -0.2784],\n",
      "          [-0.2347, -0.2519, -0.2214,  ..., -0.2596, -0.2755, -0.1875]],\n",
      "\n",
      "         [[-0.0221,  0.2241, -0.1562,  ..., -0.0943,  0.2016,  0.5395],\n",
      "          [-0.0515,  0.1365,  0.0551,  ...,  0.2476,  0.4829,  0.4131],\n",
      "          [-0.0373, -0.0186, -0.0468,  ...,  0.0369,  0.1426,  0.3519],\n",
      "          ...,\n",
      "          [-0.1802,  0.1681, -0.1648,  ..., -0.1012,  0.1169,  0.3841],\n",
      "          [-0.1322,  0.0741, -0.1399,  ..., -0.1469, -0.0150,  0.1572],\n",
      "          [-0.1199, -0.1400, -0.2426,  ..., -0.1859, -0.1129, -0.2579]],\n",
      "\n",
      "         [[-0.0983,  0.0179, -0.0752,  ..., -0.0688,  0.0105,  0.0452],\n",
      "          [ 0.1013,  0.3230,  0.2087,  ...,  0.2262,  0.3802,  0.2180],\n",
      "          [ 0.0858,  0.3287,  0.3201,  ...,  0.2792,  0.3405,  0.2208],\n",
      "          ...,\n",
      "          [-0.0709,  0.1240,  0.2618,  ...,  0.3052,  0.2874,  0.2921],\n",
      "          [-0.0355,  0.0864,  0.1096,  ...,  0.2051,  0.2953,  0.2731],\n",
      "          [-0.1603, -0.0669, -0.0970,  ...,  0.0221,  0.0426, -0.0316]]]],\n",
      "       grad_fn=<SiluBackward0>)\n",
      "<built-in method type of Tensor object at 0x000001784DAC24F0>\n",
      "tensor([[[[ 0.2724,  0.2286,  0.1425,  ...,  0.1739,  0.2336,  0.2504],\n",
      "          [ 0.2735,  0.3235,  0.2853,  ...,  0.2611,  0.3252,  0.3348],\n",
      "          [ 0.2315,  0.2758,  0.2457,  ...,  0.2424,  0.2548,  0.2589],\n",
      "          ...,\n",
      "          [ 0.1827,  0.2258,  0.2182,  ...,  0.1971,  0.2309,  0.2567],\n",
      "          [ 0.1997,  0.2619,  0.2200,  ...,  0.2107,  0.2562,  0.2838],\n",
      "          [ 0.1262,  0.2314,  0.1514,  ...,  0.1985,  0.2558,  0.1549]],\n",
      "\n",
      "         [[-0.1151, -0.2167, -0.2468,  ..., -0.2525, -0.2340, -0.2092],\n",
      "          [-0.0076, -0.1315, -0.1775,  ..., -0.1669, -0.0821, -0.0505],\n",
      "          [ 0.0494, -0.0855, -0.1107,  ..., -0.1194, -0.0532, -0.0464],\n",
      "          ...,\n",
      "          [ 0.1229, -0.0429, -0.0629,  ..., -0.1468, -0.1078, -0.0250],\n",
      "          [ 0.1086, -0.1022, -0.1329,  ..., -0.1583, -0.1241, -0.0360],\n",
      "          [-0.1256, -0.1888, -0.2312,  ..., -0.2142, -0.2024, -0.1739]],\n",
      "\n",
      "         [[-0.2667, -0.2314, -0.1809,  ..., -0.2332, -0.2170, -0.2499],\n",
      "          [-0.2079, -0.2675, -0.2683,  ..., -0.2335, -0.2329, -0.2401],\n",
      "          [-0.2119, -0.2598, -0.2519,  ..., -0.2362, -0.2249, -0.2474],\n",
      "          ...,\n",
      "          [-0.1879, -0.2339, -0.2582,  ..., -0.2501, -0.2633, -0.2569],\n",
      "          [-0.2392, -0.2725, -0.2550,  ..., -0.2549, -0.2739, -0.2755],\n",
      "          [-0.2156, -0.2393, -0.1945,  ..., -0.2345, -0.2703, -0.1575]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1726, -0.2026, -0.1910,  ..., -0.1940, -0.1943, -0.2006],\n",
      "          [-0.2363, -0.2722, -0.2560,  ..., -0.2443, -0.2519, -0.2653],\n",
      "          [-0.2227, -0.2598, -0.2617,  ..., -0.2596, -0.2501, -0.2747],\n",
      "          ...,\n",
      "          [-0.2244, -0.2598, -0.2583,  ..., -0.2616, -0.2356, -0.2730],\n",
      "          [-0.2176, -0.2552, -0.2247,  ..., -0.2515, -0.2105, -0.2617],\n",
      "          [-0.1830, -0.2166, -0.1900,  ..., -0.2113, -0.1861, -0.2476]],\n",
      "\n",
      "         [[-0.2698, -0.2775, -0.2729,  ..., -0.2579, -0.2721, -0.2779],\n",
      "          [-0.2702, -0.2236, -0.2192,  ..., -0.2764, -0.2721, -0.2694],\n",
      "          [-0.2777, -0.2624, -0.2572,  ..., -0.2745, -0.2598, -0.2700],\n",
      "          ...,\n",
      "          [-0.2522, -0.2784, -0.2739,  ..., -0.2784, -0.2541, -0.2771],\n",
      "          [-0.2497, -0.2783, -0.2729,  ..., -0.2703, -0.2678, -0.2708],\n",
      "          [-0.2556, -0.2607, -0.2680,  ..., -0.2458, -0.2684, -0.2761]],\n",
      "\n",
      "         [[-0.2229, -0.2212, -0.2047,  ..., -0.1777, -0.2053, -0.2288],\n",
      "          [-0.0920, -0.1952, -0.2201,  ..., -0.2122, -0.1988, -0.2022],\n",
      "          [-0.0805, -0.1716, -0.2092,  ..., -0.2456, -0.1915, -0.1951],\n",
      "          ...,\n",
      "          [-0.1063, -0.1829, -0.2183,  ..., -0.2055, -0.1443, -0.2200],\n",
      "          [-0.1239, -0.1943, -0.2009,  ..., -0.1941, -0.1794, -0.1971],\n",
      "          [-0.1702, -0.1782, -0.1647,  ..., -0.1544, -0.1736, -0.2041]]]],\n",
      "       grad_fn=<SiluBackward0>)\n",
      "<built-in method type of Tensor object at 0x000001784DAC2900>\n",
      "tensor([[[[-9.7946e-02,  1.6461e-01, -1.3517e-01,  ..., -8.5394e-02,\n",
      "            2.7609e-01,  2.0238e-01],\n",
      "          [ 8.3279e-03,  5.2782e-01,  2.3264e-01,  ...,  2.1137e-01,\n",
      "            6.3489e-01,  5.5497e-01],\n",
      "          [ 7.1162e-02,  3.7923e-01,  2.9709e-01,  ...,  4.3528e-01,\n",
      "            5.2406e-01,  6.3165e-01],\n",
      "          ...,\n",
      "          [ 1.0593e-01,  5.5433e-01,  4.8983e-01,  ...,  3.8913e-01,\n",
      "            5.1432e-01,  8.3373e-01],\n",
      "          [ 3.1530e-01,  6.9183e-01,  4.0612e-01,  ...,  4.5825e-01,\n",
      "            4.9192e-01,  3.9031e-01],\n",
      "          [ 4.1104e-02,  3.0180e-01, -6.5850e-02,  ...,  1.0671e-01,\n",
      "            9.6001e-02, -3.8208e-02]],\n",
      "\n",
      "         [[-2.5400e-01, -1.2186e-01,  1.1184e-02,  ..., -4.5947e-02,\n",
      "           -1.1263e-01, -6.4968e-02],\n",
      "          [-2.0022e-01, -1.7449e-01,  7.8459e-02,  ..., -1.0488e-01,\n",
      "           -2.0422e-01, -1.3466e-01],\n",
      "          [-1.5106e-01, -1.8202e-01, -8.2588e-02,  ...,  8.1692e-04,\n",
      "           -1.5571e-01,  7.1816e-03],\n",
      "          ...,\n",
      "          [-1.5466e-01, -1.6716e-01, -5.5093e-02,  ..., -6.1928e-02,\n",
      "           -2.1116e-01, -5.0758e-02],\n",
      "          [-1.6716e-01, -2.0101e-01, -1.4691e-01,  ..., -1.4271e-01,\n",
      "           -2.3788e-01, -8.7149e-02],\n",
      "          [-1.6555e-01, -1.2543e-01, -1.0432e-01,  ..., -3.7084e-02,\n",
      "           -1.8783e-01, -1.7770e-01]],\n",
      "\n",
      "         [[-2.6933e-01, -2.7798e-01, -2.7760e-01,  ..., -2.6637e-01,\n",
      "           -2.7423e-01, -2.7146e-01],\n",
      "          [-2.6994e-01, -2.7623e-01, -2.7842e-01,  ..., -2.7391e-01,\n",
      "           -2.7677e-01, -2.7621e-01],\n",
      "          [-2.7196e-01, -2.7460e-01, -2.7786e-01,  ..., -2.7255e-01,\n",
      "           -2.7781e-01, -2.7354e-01],\n",
      "          ...,\n",
      "          [-2.6876e-01, -2.7835e-01, -2.7726e-01,  ..., -2.7781e-01,\n",
      "           -2.7455e-01, -2.6307e-01],\n",
      "          [-2.6651e-01, -2.7657e-01, -2.7754e-01,  ..., -2.7704e-01,\n",
      "           -2.6193e-01, -2.6612e-01],\n",
      "          [-2.7497e-01, -2.7770e-01, -2.7807e-01,  ..., -2.7507e-01,\n",
      "           -2.6067e-01, -2.7182e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.6870e-01, -2.3122e-01, -2.4741e-01,  ..., -2.0935e-01,\n",
      "           -2.4845e-01, -2.1813e-01],\n",
      "          [-2.3255e-01, -2.4435e-01, -2.5940e-01,  ..., -2.3799e-01,\n",
      "           -2.4413e-01, -1.8184e-01],\n",
      "          [-2.3171e-01, -2.4567e-01, -2.6686e-01,  ..., -2.3653e-01,\n",
      "           -2.4965e-01, -1.6063e-01],\n",
      "          ...,\n",
      "          [-1.8974e-01, -2.1662e-01, -2.5781e-01,  ..., -2.5101e-01,\n",
      "           -2.5929e-01, -1.6365e-01],\n",
      "          [-2.1406e-01, -2.5129e-01, -2.6254e-01,  ..., -2.5206e-01,\n",
      "           -2.4732e-01, -1.8024e-01],\n",
      "          [-2.2907e-01, -2.2730e-01, -2.3802e-01,  ..., -1.9311e-01,\n",
      "           -2.5399e-01, -6.1792e-02]],\n",
      "\n",
      "         [[ 5.4324e-01,  3.1882e-01,  3.8405e-01,  ...,  3.3503e-01,\n",
      "            3.3661e-01,  4.6106e-01],\n",
      "          [ 3.1090e-01,  3.2540e-01,  3.1425e-01,  ...,  4.3606e-01,\n",
      "            3.4645e-01,  2.9714e-01],\n",
      "          [ 3.3762e-01,  3.3063e-01,  3.0889e-01,  ...,  4.0052e-01,\n",
      "            3.7673e-01,  2.9135e-01],\n",
      "          ...,\n",
      "          [ 3.6108e-01,  2.4499e-01,  3.3691e-01,  ...,  3.6578e-01,\n",
      "            3.2929e-01,  3.6101e-01],\n",
      "          [ 2.8125e-01,  1.8701e-01,  2.6434e-01,  ...,  2.8593e-01,\n",
      "            2.6893e-01,  3.5811e-01],\n",
      "          [ 5.7858e-01,  3.2544e-01,  4.3046e-01,  ...,  4.3432e-01,\n",
      "            4.1903e-01,  6.1209e-01]],\n",
      "\n",
      "         [[ 1.5662e-01,  1.3440e-01,  1.1761e-01,  ...,  1.2381e-01,\n",
      "            1.2785e-01,  1.4336e-01],\n",
      "          [ 1.1318e-01,  1.0662e-01,  9.0958e-02,  ...,  1.3715e-01,\n",
      "            1.2353e-01,  1.1748e-01],\n",
      "          [ 1.3106e-01,  1.2816e-01,  9.4288e-02,  ...,  1.2439e-01,\n",
      "            1.2711e-01,  1.0512e-01],\n",
      "          ...,\n",
      "          [ 1.1551e-01,  1.1154e-01,  1.0333e-01,  ...,  1.1055e-01,\n",
      "            1.3025e-01,  1.2923e-01],\n",
      "          [ 1.3568e-01,  1.1813e-01,  1.0653e-01,  ...,  1.1241e-01,\n",
      "            1.3449e-01,  1.3444e-01],\n",
      "          [ 1.3825e-01,  1.0488e-01,  1.0730e-01,  ...,  8.0065e-02,\n",
      "            1.2156e-01,  1.1349e-01]]]], grad_fn=<SiluBackward0>)\n",
      "<built-in method type of Tensor object at 0x000001784DAC2F90>\n"
     ]
    }
   ],
   "source": [
    "#pyt version\n",
    "effnet.eval()\n",
    "y_t = effnet(x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa360d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pre_conv1', 'conv1', 'bn1', 'activation1', 'block_0', 'block_1', 'block_2', 'block_3', 'block_4', 'block_5', 'block_6', 'block_7', 'block_8', 'block_9', 'block_10', 'block_11', 'block_12', 'block_13', 'block_14', 'block_15', 'final_conv_and_swish'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effnet.outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01a3889d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['top_bn', 'top_activation', 'avg_pool', 'top_dropout', 'probs']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(features.keys())[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f285feda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "885"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_keras = features['probs'][0]\n",
    "print(test_keras.shape)\n",
    "np.argmax(test_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ffeab38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000])\n",
      "tensor(539)\n"
     ]
    }
   ],
   "source": [
    "# test_pyt = effnet.block_list[0].outputs['depth_conv'][0].permute(1,2,0)[0,0:5,0:5]\n",
    "# test_pyt = effnet.outputs['final_conv_and_swish']\n",
    "test_pyt = y_t[0]\n",
    "print(test_pyt.shape)\n",
    "print(torch.argmax(test_pyt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b86899f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([121., 263., 171., 117.,  76.,  59.,  27.,  23.,  17.,  19.,  11.,\n",
       "         12.,   5.,  12.,   9.,   0.,   6.,   7.,   3.,   3.]),\n",
       " array([0.    , 0.0002, 0.0004, 0.0006, 0.0008, 0.001 , 0.0012, 0.0014,\n",
       "        0.0016, 0.0018, 0.002 , 0.0022, 0.0024, 0.0026, 0.0028, 0.003 ,\n",
       "        0.0032, 0.0034, 0.0036, 0.0038, 0.004 ]),\n",
       " <BarContainer object of 20 artists>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(test_keras, range=(0,0.004),bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c770f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([153., 251., 179., 104.,  65.,  54.,  34.,  19.,  18.,  18.,  18.,\n",
       "         11.,   7.,   8.,   7.,   4.,   4.,   4.,   1.,   8.]),\n",
       " array([0.    , 0.0002, 0.0004, 0.0006, 0.0008, 0.001 , 0.0012, 0.0014,\n",
       "        0.0016, 0.0018, 0.002 , 0.0022, 0.0024, 0.0026, 0.0028, 0.003 ,\n",
       "        0.0032, 0.0034, 0.0036, 0.0038, 0.004 ]),\n",
       " <BarContainer object of 20 artists>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(test_pyt.detach().numpy(), range=(0,0.004),bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "902f8404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 15 in keras:  [885 539 446 844 892 911 549 852  21  23 777 305 741 921 977]\n",
      "top 15 in pytorch:  [539 549 844 885 446 892 852 741 921  21 911 700 904 600 777]\n",
      "there are 12 common indices in the top 15:  {741, 549, 777, 844, 911, 852, 885, 21, 921, 539, 892, 446}\n"
     ]
    }
   ],
   "source": [
    "# final layer is a probability layer, size (1000,), so this is to look at the top probabilities\n",
    "top_15_keras = np.argsort(test_keras)[::-1][0:15]\n",
    "top_15_pytorch = np.argsort(test_pyt.detach().numpy())[::-1][0:15]\n",
    "print('top 15 in keras: ', top_15_keras)\n",
    "print('top 15 in pytorch: ', top_15_pytorch)\n",
    "top_15_intersection = set(top_15_keras).intersection(set(top_15_pytorch))\n",
    "print(f'there are {len(top_15_intersection)} common indices in the top 15: ', top_15_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c67943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_pad(input_shape, kernel_size):\n",
    "    \"\"\"Returns a tuple for zero-padding for 2D convolution with downsampling.\n",
    "    Args:\n",
    "      input_shape: Input img shape\n",
    "      kernel_size: An integer or tuple/list of 2 integers.\n",
    "    Returns:\n",
    "      A tuple.\n",
    "    \"\"\"\n",
    "    img_dim = 2\n",
    "    input_size = list(input_shape)[img_dim: (img_dim + 2)]\n",
    "    # 128, 128\n",
    "    if type(kernel_size):\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    # 3, 3\n",
    "    if input_size[0] is None:\n",
    "        adjust = (1, 1)\n",
    "    else:\n",
    "        adjust = (1 - input_size[0] % 2, 1 - input_size[1] % 2)\n",
    "    correct = (kernel_size[0] // 2, kernel_size[1] // 2)\n",
    "    return (\n",
    "        correct[1] - adjust[1], correct[1],\n",
    "        correct[0] - adjust[0], correct[0],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1417fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((0, 1), (0, 1)), (0, 1, 0, 1))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (top, bottom) (left, right)\n",
    "imagenet_utils.correct_pad(x,3), correct_pad([1, 3, 224, 224],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "847914f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3) torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(224,224,3)\n",
    "# x = np.ones((224,224,3))\n",
    "x = np.array([x])\n",
    "x_t = torch.from_numpy(x)\n",
    "x_t = x_t.permute(0,3,1,2).float()\n",
    "print(x.shape, x_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91ada641",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = layers.Rescaling(1.0 / 255.0)(x)\n",
    "x1 = layers.Normalization(axis=1)(x1)\n",
    "a,b,c,d = correct_pad(x1.shape, 3)\n",
    "# (top, bottom) (left, right) for padding argument\n",
    "x1 = layers.ZeroPadding2D(\n",
    "        padding=imagenet_utils.correct_pad(x,3), data_format='channels_last',name=\"stem_conv_pad\")(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d29ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale = lambda x: x * 1./255\n",
    "pyt_transforms = transforms.Compose([\n",
    "            transforms.Lambda(rescale),\n",
    "            transforms.Normalize(0, 1),\n",
    "            # padding left, right, top, bottom\n",
    "            transforms.Lambda(nn.ZeroPad2d(padding=correct_pad([1, 3, 224, 224],3))),]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "852143ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t1 = pyt_transforms(x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "363fe5d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 225, 225, 3]), torch.Size([1, 3, 225, 225]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape, x_t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1e1bd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       " array([[7.8995910e-04, 2.5729151e-03, 6.5443492e-05],\n",
       "        [1.3964709e-03, 6.1202224e-04, 3.8473203e-03],\n",
       "        [2.4883827e-04, 4.4113860e-04, 1.9027198e-03]], dtype=float32)>,\n",
       " tensor([[7.8996e-04, 2.5729e-03, 6.5443e-05],\n",
       "         [1.3965e-03, 6.1202e-04, 3.8473e-03],\n",
       "         [2.4884e-04, 4.4114e-04, 1.9027e-03]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1[0][0:3,0:3,0], x_t1[0].permute(1,2,0)[0:3,0:3,0],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "693f63d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras weights:  (3, 3, 3, 32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "padding='same' is not supported for strided convolutions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeras weights: \u001b[39m\u001b[38;5;124m'\u001b[39m,weights\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      3\u001b[0m keras_conv \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m3\u001b[39m, strides\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, weights\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([weights]),data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannels_last\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m pyt_conv \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msame\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m sd \u001b[38;5;241m=\u001b[39m pyt_conv\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[0;32m      6\u001b[0m weights_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mmoveaxis(weights, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]))\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_effnet\\lib\\site-packages\\torch\\nn\\modules\\conv.py:434\u001b[0m, in \u001b[0;36mConv2d.__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[0;32m    432\u001b[0m padding_ \u001b[38;5;241m=\u001b[39m padding \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m _pair(padding)\n\u001b[0;32m    433\u001b[0m dilation_ \u001b[38;5;241m=\u001b[39m _pair(dilation)\n\u001b[1;32m--> 434\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mConv2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_effnet\\lib\\site-packages\\torch\\nn\\modules\\conv.py:94\u001b[0m, in \u001b[0;36m_ConvNd.__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     91\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid padding string \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m, should be one of \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     92\u001b[0m                 padding, valid_padding_strings))\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m padding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(s \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m stride):\n\u001b[1;32m---> 94\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpadding=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not supported for strided convolutions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     96\u001b[0m valid_padding_modes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreflect\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplicate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcircular\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m valid_padding_modes:\n",
      "\u001b[1;31mValueError\u001b[0m: padding='same' is not supported for strided convolutions"
     ]
    }
   ],
   "source": [
    "weights = model.layers[1].get_weights()[0]\n",
    "print('keras weights: ',weights.shape)\n",
    "keras_conv = layers.Conv2D(32, 3, strides=2, padding=\"same\", use_bias=False, weights=np.array([weights]),data_format=\"channels_last\")\n",
    "pyt_conv = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=\"same\", bias=False)\n",
    "sd = pyt_conv.state_dict()\n",
    "weights_t = torch.from_numpy(np.moveaxis(weights, [-1, -2], [0, 1]))\n",
    "# weights_t = torch.from_numpy(weights.T)\n",
    "print('torch weights: ',weights_t.shape)\n",
    "sd['weight']= weights_t \n",
    "pyt_conv.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa993fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = keras_conv(x1)[0]\n",
    "x_t1 = pyt_conv(x_t1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d025476",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.shape, x_t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a296a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1[0:5,0:5,1], x_t1.permute(1,2,0)[0:5,0:5,1],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeca0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.randn(1, 526, 140, 140)\n",
    "# (2*(output-1) - input - 3)*(1 / 2)\n",
    "print((2*(140-1) - 140 - 3)*(1 / 2))\n",
    "print(x.shape)\n",
    "x = F.pad(x, (71, 70, 71, 70), )\n",
    "print(x.shape)\n",
    "x = nn.Conv2d(526, 64, 3, 2)(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f4311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_height, in_width = 100,20\n",
    "filter_height, filter_width = 4,1\n",
    "strides=(None,1,1)\n",
    "out_height = np.ceil(float(in_height) / float(strides[1]))\n",
    "out_width  = np.ceil(float(in_width) / float(strides[2]))\n",
    "\n",
    "#The total padding applied along the height and width is computed as:\n",
    "\n",
    "if (in_height % strides[1] == 0):\n",
    "  pad_along_height = max(filter_height - strides[1], 0)\n",
    "else:\n",
    "  pad_along_height = max(filter_height - (in_height % strides[1]), 0)\n",
    "if (in_width % strides[2] == 0):\n",
    "  pad_along_width = max(filter_width - strides[2], 0)\n",
    "else:\n",
    "  pad_along_width = max(filter_width - (in_width % strides[2]), 0)\n",
    "\n",
    "print(pad_along_height, pad_along_width)\n",
    "  \n",
    "#Finally, the padding on the top, bottom, left and right are:\n",
    "\n",
    "pad_top = pad_along_height // 2\n",
    "pad_bottom = pad_along_height - pad_top\n",
    "pad_left = pad_along_width // 2\n",
    "pad_right = pad_along_width - pad_left\n",
    "\n",
    "print(pad_left, pad_right, pad_top, pad_bottom)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
